{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72654dd8-e761-4d6c-999c-4063c88650fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "NAME = \"01_baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6064f6-321a-4f3a-8c93-8cce10fa4396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 151010689]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131bb0cf-bb6c-4e6c-aa3c-194297a1e0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(\n",
    "    epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=params.replay_size)\n",
    "optimizer = optim.Adam(net.parameters(),\n",
    "                       lr = params.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6758035-3898-4f62-81da-04151b9da2c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-20, steps=1022, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 2: reward=-20, steps=951, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 3: reward=-19, steps=945, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 4: reward=-18, steps=1026, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 5: reward=-21, steps=836, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 6: reward=-20, steps=1025, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 7: reward=-21, steps=983, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 8: reward=-20, steps=913, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 9: reward=-21, steps=837, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 10: reward=-21, steps=942, speed=0.0 f/s, elapsed=0:01:03\n",
      "Episode 11: reward=-20, steps=988, speed=71.4 f/s, elapsed=0:01:09\n",
      "Episode 12: reward=-20, steps=950, speed=71.4 f/s, elapsed=0:01:23\n",
      "Episode 13: reward=-20, steps=960, speed=71.4 f/s, elapsed=0:01:36\n",
      "Episode 14: reward=-20, steps=837, speed=71.4 f/s, elapsed=0:01:48\n",
      "Episode 15: reward=-20, steps=862, speed=71.3 f/s, elapsed=0:02:00\n",
      "Episode 16: reward=-21, steps=876, speed=71.4 f/s, elapsed=0:02:13\n",
      "Episode 17: reward=-20, steps=971, speed=71.3 f/s, elapsed=0:02:27\n",
      "Episode 18: reward=-21, steps=840, speed=71.3 f/s, elapsed=0:02:39\n",
      "Episode 19: reward=-20, steps=838, speed=71.3 f/s, elapsed=0:02:51\n",
      "Episode 20: reward=-21, steps=844, speed=71.3 f/s, elapsed=0:03:02\n",
      "Episode 21: reward=-21, steps=811, speed=71.2 f/s, elapsed=0:03:14\n",
      "Episode 22: reward=-19, steps=1060, speed=71.2 f/s, elapsed=0:03:29\n",
      "Episode 23: reward=-19, steps=975, speed=71.2 f/s, elapsed=0:03:43\n",
      "Episode 24: reward=-20, steps=982, speed=71.1 f/s, elapsed=0:03:57\n",
      "Episode 25: reward=-21, steps=778, speed=71.1 f/s, elapsed=0:04:08\n",
      "Episode 26: reward=-20, steps=1004, speed=71.1 f/s, elapsed=0:04:23\n",
      "Episode 27: reward=-21, steps=846, speed=71.1 f/s, elapsed=0:04:35\n",
      "Episode 28: reward=-20, steps=1242, speed=71.1 f/s, elapsed=0:04:52\n",
      "Episode 29: reward=-19, steps=1427, speed=71.1 f/s, elapsed=0:05:13\n",
      "Episode 30: reward=-17, steps=1318, speed=71.0 f/s, elapsed=0:05:32\n",
      "Episode 31: reward=-20, steps=973, speed=71.0 f/s, elapsed=0:05:46\n",
      "Episode 32: reward=-19, steps=1240, speed=71.0 f/s, elapsed=0:06:03\n",
      "Episode 33: reward=-21, steps=921, speed=71.0 f/s, elapsed=0:06:17\n",
      "Episode 34: reward=-18, steps=1267, speed=70.9 f/s, elapsed=0:06:35\n",
      "Episode 35: reward=-18, steps=1241, speed=70.9 f/s, elapsed=0:06:52\n",
      "Episode 36: reward=-20, steps=1094, speed=70.9 f/s, elapsed=0:07:08\n",
      "Episode 37: reward=-21, steps=1169, speed=70.9 f/s, elapsed=0:07:25\n",
      "Episode 38: reward=-17, steps=1397, speed=70.9 f/s, elapsed=0:07:45\n",
      "Episode 39: reward=-21, steps=1166, speed=70.9 f/s, elapsed=0:08:01\n",
      "Episode 40: reward=-18, steps=1126, speed=70.8 f/s, elapsed=0:08:17\n",
      "Episode 41: reward=-18, steps=1333, speed=70.8 f/s, elapsed=0:08:37\n",
      "Episode 42: reward=-18, steps=1263, speed=70.7 f/s, elapsed=0:08:56\n",
      "Episode 43: reward=-19, steps=1126, speed=70.6 f/s, elapsed=0:09:12\n",
      "Episode 44: reward=-18, steps=1438, speed=70.5 f/s, elapsed=0:09:34\n",
      "Episode 45: reward=-15, steps=1476, speed=70.4 f/s, elapsed=0:09:57\n",
      "Episode 46: reward=-19, steps=1523, speed=70.4 f/s, elapsed=0:10:19\n",
      "Episode 47: reward=-19, steps=1284, speed=70.4 f/s, elapsed=0:10:37\n",
      "Episode 48: reward=-16, steps=1556, speed=70.4 f/s, elapsed=0:10:59\n",
      "Episode 49: reward=-16, steps=1554, speed=70.4 f/s, elapsed=0:11:22\n",
      "Episode 50: reward=-17, steps=1290, speed=70.3 f/s, elapsed=0:11:42\n",
      "Episode 51: reward=-18, steps=1458, speed=70.2 f/s, elapsed=0:12:04\n",
      "Episode 52: reward=-16, steps=1367, speed=70.1 f/s, elapsed=0:12:25\n",
      "Episode 53: reward=-17, steps=1765, speed=70.0 f/s, elapsed=0:12:51\n",
      "Episode 54: reward=-14, steps=1758, speed=69.9 f/s, elapsed=0:13:18\n",
      "Episode 55: reward=-19, steps=1479, speed=69.8 f/s, elapsed=0:13:41\n",
      "Episode 56: reward=-12, steps=2115, speed=69.8 f/s, elapsed=0:14:13\n",
      "Episode 57: reward=-18, steps=1283, speed=69.8 f/s, elapsed=0:14:31\n",
      "Episode 58: reward=-19, steps=1262, speed=69.8 f/s, elapsed=0:14:49\n",
      "Episode 59: reward=-17, steps=1533, speed=69.7 f/s, elapsed=0:15:13\n",
      "Episode 60: reward=-15, steps=1681, speed=69.7 f/s, elapsed=0:15:37\n",
      "Episode 61: reward=-17, steps=1697, speed=69.7 f/s, elapsed=0:16:01\n",
      "Episode 62: reward=-15, steps=1703, speed=69.6 f/s, elapsed=0:16:26\n",
      "Episode 63: reward=-15, steps=1703, speed=69.6 f/s, elapsed=0:16:51\n",
      "Episode 64: reward=-14, steps=2195, speed=69.6 f/s, elapsed=0:17:23\n",
      "Episode 65: reward=-14, steps=2144, speed=69.6 f/s, elapsed=0:17:53\n",
      "Episode 66: reward=-15, steps=2119, speed=69.6 f/s, elapsed=0:18:24\n",
      "Episode 67: reward=-14, steps=2238, speed=69.6 f/s, elapsed=0:18:56\n",
      "Episode 68: reward=-19, steps=1388, speed=69.6 f/s, elapsed=0:19:15\n",
      "Episode 69: reward=-14, steps=2377, speed=69.6 f/s, elapsed=0:19:50\n",
      "Episode 70: reward=-12, steps=2247, speed=69.7 f/s, elapsed=0:20:22\n",
      "Episode 71: reward=-18, steps=2140, speed=69.7 f/s, elapsed=0:20:52\n",
      "Episode 72: reward=-15, steps=2306, speed=69.7 f/s, elapsed=0:21:25\n",
      "Episode 73: reward=-19, steps=1917, speed=69.6 f/s, elapsed=0:21:54\n",
      "Episode 74: reward=-9, steps=3090, speed=69.6 f/s, elapsed=0:22:38\n",
      "Episode 75: reward=-15, steps=2633, speed=69.6 f/s, elapsed=0:23:15\n",
      "Episode 76: reward=-16, steps=2139, speed=69.6 f/s, elapsed=0:23:46\n",
      "Episode 77: reward=-14, steps=2529, speed=69.6 f/s, elapsed=0:24:22\n",
      "Episode 78: reward=-7, steps=3197, speed=69.6 f/s, elapsed=0:25:08\n",
      "Episode 79: reward=-16, steps=2454, speed=69.6 f/s, elapsed=0:25:43\n",
      "Episode 80: reward=-7, steps=3280, speed=69.7 f/s, elapsed=0:26:30\n",
      "Episode 81: reward=-12, steps=2718, speed=69.7 f/s, elapsed=0:27:09\n",
      "Episode 82: reward=-13, steps=2764, speed=69.7 f/s, elapsed=0:27:49\n",
      "Episode 83: reward=-6, steps=3099, speed=69.7 f/s, elapsed=0:28:33\n",
      "Episode 84: reward=-10, steps=3167, speed=69.7 f/s, elapsed=0:29:18\n",
      "Episode 85: reward=-2, steps=4059, speed=69.7 f/s, elapsed=0:30:16\n",
      "Episode 86: reward=3, steps=3530, speed=69.7 f/s, elapsed=0:31:07\n",
      "Episode 87: reward=-6, steps=3605, speed=69.7 f/s, elapsed=0:31:59\n",
      "Episode 88: reward=-10, steps=2908, speed=69.7 f/s, elapsed=0:32:40\n",
      "Episode 89: reward=-2, steps=3627, speed=69.7 f/s, elapsed=0:33:32\n",
      "Episode 90: reward=-12, steps=2958, speed=69.7 f/s, elapsed=0:34:15\n",
      "Episode 91: reward=-4, steps=3993, speed=69.7 f/s, elapsed=0:35:12\n",
      "Episode 92: reward=-11, steps=2772, speed=69.7 f/s, elapsed=0:35:51\n",
      "Episode 93: reward=-11, steps=2784, speed=69.7 f/s, elapsed=0:36:32\n",
      "Episode 94: reward=-2, steps=3525, speed=69.7 f/s, elapsed=0:37:22\n",
      "Episode 95: reward=-3, steps=4036, speed=69.7 f/s, elapsed=0:38:20\n",
      "Episode 96: reward=-3, steps=4065, speed=69.7 f/s, elapsed=0:39:18\n",
      "Episode 97: reward=-7, steps=2866, speed=69.7 f/s, elapsed=0:39:59\n",
      "Episode 98: reward=1, steps=4162, speed=69.7 f/s, elapsed=0:40:59\n",
      "Episode 99: reward=2, steps=4512, speed=69.7 f/s, elapsed=0:42:04\n",
      "Episode 100: reward=-1, steps=4735, speed=69.7 f/s, elapsed=0:43:11\n",
      "Episode 101: reward=-5, steps=3933, speed=69.7 f/s, elapsed=0:44:08\n",
      "Episode 102: reward=-7, steps=3366, speed=69.7 f/s, elapsed=0:44:56\n",
      "Episode 103: reward=-6, steps=4230, speed=69.7 f/s, elapsed=0:45:56\n",
      "Episode 104: reward=5, steps=4535, speed=69.7 f/s, elapsed=0:47:04\n",
      "Episode 105: reward=2, steps=3827, speed=69.6 f/s, elapsed=0:48:02\n",
      "Episode 106: reward=2, steps=3871, speed=69.5 f/s, elapsed=0:48:59\n",
      "Episode 107: reward=-1, steps=4339, speed=69.4 f/s, elapsed=0:50:08\n",
      "Episode 108: reward=1, steps=4200, speed=69.4 f/s, elapsed=0:51:11\n",
      "Episode 109: reward=1, steps=4092, speed=69.1 f/s, elapsed=0:52:23\n",
      "Episode 110: reward=2, steps=3592, speed=68.8 f/s, elapsed=0:53:28\n",
      "Episode 111: reward=-2, steps=3788, speed=68.6 f/s, elapsed=0:54:33\n",
      "Episode 112: reward=-1, steps=4255, speed=68.4 f/s, elapsed=0:55:45\n",
      "Episode 113: reward=-3, steps=3832, speed=68.2 f/s, elapsed=0:56:50\n",
      "Episode 114: reward=-3, steps=3846, speed=68.0 f/s, elapsed=0:57:59\n",
      "Episode 115: reward=4, steps=3947, speed=67.8 f/s, elapsed=0:59:09\n",
      "Episode 116: reward=8, steps=3544, speed=67.5 f/s, elapsed=1:00:12\n",
      "Episode 117: reward=7, steps=3992, speed=67.3 f/s, elapsed=1:01:23\n",
      "Episode 118: reward=6, steps=3936, speed=67.1 f/s, elapsed=1:02:32\n",
      "Episode 119: reward=8, steps=3361, speed=66.9 f/s, elapsed=1:03:32\n",
      "Episode 120: reward=-8, steps=3360, speed=66.7 f/s, elapsed=1:04:32\n",
      "Episode 121: reward=3, steps=3597, speed=66.5 f/s, elapsed=1:05:35\n",
      "Episode 122: reward=8, steps=3334, speed=66.4 f/s, elapsed=1:06:26\n",
      "Episode 123: reward=-9, steps=2895, speed=66.5 f/s, elapsed=1:07:08\n",
      "Episode 124: reward=-1, steps=3700, speed=66.6 f/s, elapsed=1:08:01\n",
      "Episode 125: reward=6, steps=3325, speed=66.6 f/s, elapsed=1:08:49\n",
      "Episode 126: reward=7, steps=4328, speed=66.7 f/s, elapsed=1:09:51\n",
      "Episode 127: reward=9, steps=2936, speed=66.8 f/s, elapsed=1:10:33\n",
      "Episode 128: reward=4, steps=3626, speed=66.8 f/s, elapsed=1:11:25\n",
      "Episode 129: reward=2, steps=3928, speed=66.9 f/s, elapsed=1:12:21\n",
      "Episode 130: reward=4, steps=3730, speed=66.9 f/s, elapsed=1:13:15\n",
      "Episode 131: reward=18, steps=1964, speed=67.0 f/s, elapsed=1:13:43\n",
      "Episode 132: reward=11, steps=2559, speed=67.0 f/s, elapsed=1:14:20\n",
      "Episode 133: reward=15, steps=2512, speed=67.1 f/s, elapsed=1:14:56\n",
      "Episode 134: reward=9, steps=2960, speed=67.1 f/s, elapsed=1:15:38\n",
      "Episode 135: reward=7, steps=2640, speed=67.2 f/s, elapsed=1:16:16\n",
      "Episode 136: reward=12, steps=3250, speed=67.2 f/s, elapsed=1:17:03\n",
      "Episode 137: reward=-2, steps=3655, speed=67.3 f/s, elapsed=1:17:56\n",
      "Episode 138: reward=9, steps=2990, speed=67.3 f/s, elapsed=1:18:39\n",
      "Episode 139: reward=11, steps=2591, speed=67.4 f/s, elapsed=1:19:16\n",
      "Episode 140: reward=5, steps=3417, speed=67.4 f/s, elapsed=1:20:05\n",
      "Episode 141: reward=11, steps=2686, speed=67.5 f/s, elapsed=1:20:44\n",
      "Episode 142: reward=14, steps=2647, speed=67.5 f/s, elapsed=1:21:22\n",
      "Episode 143: reward=7, steps=3108, speed=67.5 f/s, elapsed=1:22:06\n",
      "Episode 144: reward=11, steps=2785, speed=67.6 f/s, elapsed=1:22:46\n",
      "Episode 145: reward=9, steps=2983, speed=67.6 f/s, elapsed=1:23:29\n",
      "Episode 146: reward=12, steps=2712, speed=67.6 f/s, elapsed=1:24:08\n",
      "Episode 147: reward=7, steps=3432, speed=67.7 f/s, elapsed=1:24:58\n",
      "Episode 148: reward=13, steps=2726, speed=67.7 f/s, elapsed=1:25:37\n",
      "Episode 149: reward=18, steps=1899, speed=67.8 f/s, elapsed=1:26:04\n",
      "Episode 150: reward=12, steps=2800, speed=67.8 f/s, elapsed=1:26:44\n",
      "Episode 151: reward=13, steps=2403, speed=67.8 f/s, elapsed=1:27:19\n",
      "Episode 152: reward=-3, steps=3757, speed=67.9 f/s, elapsed=1:28:13\n",
      "Episode 153: reward=8, steps=2830, speed=67.9 f/s, elapsed=1:28:53\n",
      "Episode 154: reward=14, steps=2466, speed=67.9 f/s, elapsed=1:29:29\n",
      "Episode 155: reward=10, steps=2922, speed=68.0 f/s, elapsed=1:30:11\n",
      "Episode 156: reward=18, steps=2004, speed=68.0 f/s, elapsed=1:30:40\n",
      "Episode 157: reward=15, steps=2451, speed=68.0 f/s, elapsed=1:31:15\n",
      "Episode 158: reward=18, steps=2091, speed=68.1 f/s, elapsed=1:31:45\n",
      "Episode 159: reward=17, steps=2008, speed=68.1 f/s, elapsed=1:32:14\n",
      "Episode 160: reward=15, steps=2332, speed=68.1 f/s, elapsed=1:32:47\n",
      "Episode 161: reward=20, steps=1709, speed=68.2 f/s, elapsed=1:33:12\n",
      "Episode 162: reward=16, steps=2207, speed=68.2 f/s, elapsed=1:33:44\n",
      "Episode 163: reward=14, steps=2320, speed=68.1 f/s, elapsed=1:34:19\n",
      "Episode 164: reward=17, steps=2570, speed=68.1 f/s, elapsed=1:34:57\n",
      "Episode 165: reward=14, steps=2328, speed=68.2 f/s, elapsed=1:35:30\n",
      "Episode 166: reward=20, steps=1790, speed=68.2 f/s, elapsed=1:35:56\n",
      "Episode 167: reward=15, steps=2271, speed=68.2 f/s, elapsed=1:36:28\n",
      "Episode 168: reward=12, steps=2374, speed=68.3 f/s, elapsed=1:37:02\n",
      "Episode 169: reward=19, steps=2079, speed=68.3 f/s, elapsed=1:37:32\n",
      "Episode 170: reward=18, steps=1856, speed=68.3 f/s, elapsed=1:37:59\n",
      "Episode 171: reward=18, steps=1901, speed=68.3 f/s, elapsed=1:38:26\n",
      "Episode 172: reward=18, steps=2212, speed=68.4 f/s, elapsed=1:38:57\n",
      "Episode 173: reward=14, steps=2765, speed=68.4 f/s, elapsed=1:39:37\n",
      "Episode 174: reward=19, steps=2189, speed=68.4 f/s, elapsed=1:40:09\n",
      "Episode 175: reward=13, steps=2501, speed=68.5 f/s, elapsed=1:40:45\n",
      "Episode 176: reward=20, steps=1792, speed=68.5 f/s, elapsed=1:41:10\n",
      "Episode 177: reward=18, steps=1905, speed=68.5 f/s, elapsed=1:41:38\n",
      "Episode 178: reward=21, steps=1635, speed=68.5 f/s, elapsed=1:42:01\n",
      "Episode 179: reward=14, steps=2316, speed=68.5 f/s, elapsed=1:42:34\n",
      "Episode 180: reward=20, steps=1948, speed=68.6 f/s, elapsed=1:43:02\n",
      "Episode 181: reward=19, steps=1808, speed=68.6 f/s, elapsed=1:43:28\n",
      "Episode 182: reward=20, steps=1665, speed=68.6 f/s, elapsed=1:43:52\n",
      "Episode 183: reward=20, steps=1770, speed=68.7 f/s, elapsed=1:44:17\n",
      "Episode 184: reward=20, steps=1906, speed=68.7 f/s, elapsed=1:44:45\n",
      "Episode 185: reward=18, steps=1881, speed=68.7 f/s, elapsed=1:45:12\n",
      "Episode 186: reward=21, steps=1718, speed=68.7 f/s, elapsed=1:45:36\n",
      "Episode 187: reward=18, steps=1783, speed=68.7 f/s, elapsed=1:46:02\n",
      "Episode 188: reward=21, steps=1638, speed=68.7 f/s, elapsed=1:46:26\n",
      "Episode 189: reward=16, steps=2051, speed=68.7 f/s, elapsed=1:46:55\n",
      "Episode 190: reward=18, steps=1826, speed=68.8 f/s, elapsed=1:47:21\n",
      "Episode 191: reward=20, steps=1753, speed=68.8 f/s, elapsed=1:47:46\n",
      "Episode 192: reward=19, steps=1866, speed=68.8 f/s, elapsed=1:48:13\n",
      "Episode 193: reward=18, steps=1890, speed=68.8 f/s, elapsed=1:48:40\n",
      "Episode 194: reward=20, steps=1824, speed=68.9 f/s, elapsed=1:49:06\n",
      "Episode 195: reward=16, steps=2286, speed=68.9 f/s, elapsed=1:49:39\n",
      "Episode 196: reward=20, steps=1931, speed=68.9 f/s, elapsed=1:50:07\n",
      "Episode 197: reward=19, steps=1799, speed=68.9 f/s, elapsed=1:50:33\n",
      "Episode 198: reward=14, steps=2341, speed=68.9 f/s, elapsed=1:51:06\n",
      "Episode 199: reward=16, steps=2319, speed=68.9 f/s, elapsed=1:51:40\n",
      "Episode 200: reward=20, steps=1791, speed=68.9 f/s, elapsed=1:52:05\n",
      "Episode 201: reward=17, steps=2144, speed=68.9 f/s, elapsed=1:52:36\n",
      "Episode 202: reward=12, steps=2639, speed=69.0 f/s, elapsed=1:53:14\n",
      "Episode 203: reward=17, steps=2084, speed=69.0 f/s, elapsed=1:53:44\n",
      "Episode 204: reward=18, steps=2021, speed=69.0 f/s, elapsed=1:54:13\n",
      "Episode 205: reward=13, steps=2799, speed=69.0 f/s, elapsed=1:54:53\n",
      "Episode 206: reward=19, steps=1929, speed=69.0 f/s, elapsed=1:55:20\n",
      "Episode 207: reward=19, steps=1781, speed=69.1 f/s, elapsed=1:55:46\n",
      "Episode 208: reward=18, steps=1950, speed=69.1 f/s, elapsed=1:56:14\n",
      "Episode 209: reward=16, steps=2094, speed=69.1 f/s, elapsed=1:56:44\n",
      "Episode 210: reward=20, steps=1884, speed=69.1 f/s, elapsed=1:57:11\n",
      "Episode 211: reward=17, steps=2051, speed=69.1 f/s, elapsed=1:57:40\n",
      "Episode 212: reward=21, steps=1639, speed=69.1 f/s, elapsed=1:58:04\n",
      "Episode 213: reward=16, steps=2418, speed=69.1 f/s, elapsed=1:58:39\n",
      "Episode 214: reward=18, steps=2036, speed=69.1 f/s, elapsed=1:59:08\n",
      "Episode 215: reward=17, steps=2029, speed=69.2 f/s, elapsed=1:59:37\n",
      "Episode 216: reward=16, steps=2199, speed=69.1 f/s, elapsed=2:00:09\n",
      "Episode 217: reward=20, steps=1930, speed=69.1 f/s, elapsed=2:00:37\n",
      "Episode 218: reward=19, steps=1953, speed=69.2 f/s, elapsed=2:01:05\n",
      "Episode 219: reward=18, steps=1996, speed=69.2 f/s, elapsed=2:01:34\n",
      "Episode 220: reward=16, steps=2315, speed=69.2 f/s, elapsed=2:02:07\n",
      "Episode 221: reward=19, steps=1931, speed=69.2 f/s, elapsed=2:02:35\n",
      "Episode 222: reward=16, steps=2220, speed=69.2 f/s, elapsed=2:03:06\n",
      "Episode 223: reward=15, steps=2017, speed=69.2 f/s, elapsed=2:03:35\n",
      "Episode 224: reward=19, steps=1870, speed=69.2 f/s, elapsed=2:04:02\n",
      "Episode 225: reward=19, steps=1806, speed=69.2 f/s, elapsed=2:04:28\n",
      "Episode 226: reward=17, steps=1988, speed=69.2 f/s, elapsed=2:04:57\n",
      "Episode 227: reward=18, steps=2042, speed=69.2 f/s, elapsed=2:05:26\n",
      "Episode 228: reward=12, steps=2398, speed=69.2 f/s, elapsed=2:06:00\n",
      "Episode 229: reward=16, steps=2330, speed=69.3 f/s, elapsed=2:06:34\n",
      "Episode 230: reward=17, steps=2136, speed=69.3 f/s, elapsed=2:07:04\n",
      "Episode 231: reward=20, steps=1818, speed=69.3 f/s, elapsed=2:07:31\n",
      "Episode 232: reward=18, steps=1956, speed=69.3 f/s, elapsed=2:07:59\n",
      "Episode 233: reward=15, steps=2356, speed=69.3 f/s, elapsed=2:08:33\n",
      "Episode 234: reward=20, steps=1725, speed=69.3 f/s, elapsed=2:08:57\n",
      "Episode 235: reward=8, steps=2556, speed=69.3 f/s, elapsed=2:09:34\n",
      "Episode 236: reward=15, steps=2202, speed=69.3 f/s, elapsed=2:10:06\n",
      "Episode 237: reward=20, steps=1746, speed=69.3 f/s, elapsed=2:10:30\n",
      "Episode 238: reward=20, steps=1781, speed=69.3 f/s, elapsed=2:10:56\n",
      "Episode 239: reward=20, steps=1783, speed=69.3 f/s, elapsed=2:11:22\n",
      "Episode 240: reward=18, steps=2038, speed=69.3 f/s, elapsed=2:11:51\n",
      "Episode 241: reward=21, steps=1873, speed=69.3 f/s, elapsed=2:12:18\n",
      "Episode 242: reward=18, steps=1945, speed=69.3 f/s, elapsed=2:12:46\n",
      "Episode 243: reward=19, steps=1879, speed=69.3 f/s, elapsed=2:13:13\n",
      "Episode 244: reward=18, steps=2103, speed=69.3 f/s, elapsed=2:13:44\n",
      "Episode 245: reward=12, steps=2705, speed=69.3 f/s, elapsed=2:14:24\n",
      "Episode 246: reward=18, steps=1964, speed=69.3 f/s, elapsed=2:14:53\n",
      "Episode 247: reward=21, steps=1718, speed=69.3 f/s, elapsed=2:15:17\n",
      "Episode 248: reward=14, steps=2739, speed=69.3 f/s, elapsed=2:15:57\n",
      "Episode 249: reward=20, steps=1691, speed=69.3 f/s, elapsed=2:16:21\n",
      "Episode 250: reward=18, steps=1821, speed=69.3 f/s, elapsed=2:16:48\n",
      "Episode 251: reward=19, steps=1826, speed=69.2 f/s, elapsed=2:17:15\n",
      "Episode 252: reward=17, steps=2566, speed=69.2 f/s, elapsed=2:17:53\n",
      "Episode 253: reward=20, steps=1828, speed=69.2 f/s, elapsed=2:18:19\n",
      "Episode 254: reward=20, steps=1685, speed=69.2 f/s, elapsed=2:18:44\n",
      "Episode 255: reward=15, steps=2205, speed=69.2 f/s, elapsed=2:19:15\n",
      "Episode 256: reward=19, steps=1931, speed=69.2 f/s, elapsed=2:19:43\n",
      "Episode 257: reward=15, steps=2178, speed=69.2 f/s, elapsed=2:20:15\n",
      "Episode 258: reward=17, steps=2036, speed=69.2 f/s, elapsed=2:20:44\n",
      "Episode 259: reward=18, steps=2162, speed=69.2 f/s, elapsed=2:21:15\n",
      "Episode 260: reward=19, steps=1888, speed=69.2 f/s, elapsed=2:21:42\n",
      "Episode 261: reward=19, steps=1949, speed=69.2 f/s, elapsed=2:22:10\n",
      "Episode 262: reward=21, steps=1656, speed=69.2 f/s, elapsed=2:22:34\n",
      "Episode 263: reward=20, steps=1927, speed=69.2 f/s, elapsed=2:23:02\n",
      "Episode 264: reward=15, steps=2357, speed=69.2 f/s, elapsed=2:23:36\n",
      "Episode 265: reward=19, steps=1917, speed=69.3 f/s, elapsed=2:24:03\n",
      "Episode 266: reward=19, steps=1916, speed=69.3 f/s, elapsed=2:24:31\n",
      "Episode 267: reward=21, steps=1633, speed=69.3 f/s, elapsed=2:24:54\n",
      "Episode 268: reward=21, steps=1666, speed=69.3 f/s, elapsed=2:25:18\n",
      "Episode 269: reward=17, steps=2257, speed=69.3 f/s, elapsed=2:25:50\n",
      "Episode 270: reward=20, steps=1778, speed=69.3 f/s, elapsed=2:26:16\n",
      "Episode 271: reward=19, steps=1799, speed=69.3 f/s, elapsed=2:26:42\n",
      "Episode 272: reward=14, steps=2430, speed=69.3 f/s, elapsed=2:27:17\n",
      "Episode 273: reward=21, steps=1635, speed=69.3 f/s, elapsed=2:27:40\n",
      "Episode 274: reward=19, steps=1785, speed=69.3 f/s, elapsed=2:28:06\n",
      "Episode 275: reward=20, steps=1703, speed=69.3 f/s, elapsed=2:28:30\n",
      "Episode 276: reward=19, steps=1828, speed=69.3 f/s, elapsed=2:28:57\n",
      "Episode 277: reward=21, steps=1695, speed=69.3 f/s, elapsed=2:29:21\n",
      "Episode 278: reward=18, steps=2265, speed=69.3 f/s, elapsed=2:29:54\n",
      "Episode 279: reward=20, steps=1702, speed=69.3 f/s, elapsed=2:30:18\n",
      "Episode 280: reward=19, steps=1861, speed=69.4 f/s, elapsed=2:30:45\n",
      "Episode 281: reward=15, steps=2297, speed=69.4 f/s, elapsed=2:31:18\n",
      "Episode 282: reward=16, steps=2044, speed=69.4 f/s, elapsed=2:31:47\n",
      "Episode 283: reward=21, steps=1634, speed=69.4 f/s, elapsed=2:32:11\n",
      "Episode 284: reward=19, steps=1937, speed=69.4 f/s, elapsed=2:32:38\n",
      "Episode 285: reward=18, steps=2067, speed=69.4 f/s, elapsed=2:33:08\n",
      "Episode 286: reward=15, steps=2264, speed=69.4 f/s, elapsed=2:33:40\n",
      "Episode 287: reward=19, steps=1848, speed=69.4 f/s, elapsed=2:34:07\n",
      "Episode 288: reward=21, steps=1652, speed=69.4 f/s, elapsed=2:34:30\n",
      "Episode 289: reward=19, steps=1785, speed=69.4 f/s, elapsed=2:34:56\n",
      "Episode 290: reward=21, steps=1640, speed=69.3 f/s, elapsed=2:35:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15440\\4110241830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_ignite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m engine.run(common.batch_generator(buffer, params.replay_initial,\n\u001b[1;32m---> 18\u001b[1;33m                                   params.batch_size))\n\u001b[0m",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                 \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                 \u001b[1;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIProjects\\DeepRL\\Unit 8\\lib\\common.py\u001b[0m in \u001b[0;36mbatch_generator\u001b[1;34m(buffer, initial, batch_size)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\experience.py\u001b[0m in \u001b[0;36mpopulate\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \"\"\"\n\u001b[0;32m    367\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mentry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_source_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mExperienceSourceFirstLast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0mlast_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\experience.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m                     \u001b[0mnext_state_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                     \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                     \u001b[0mnext_state_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mis_done\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\common\\wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwas_real_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# check current lives, make loss of life terminal,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = common.calc_loss_dqn(\n",
    "        batch, net, tgt_net.target_model, gamma=params.gamma, device=device)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    epsilon_tracker.frame(engine.state.iteration)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }\n",
    "\n",
    "engine = Engine(process_batch)\n",
    "common.setup_ignite(engine, params, exp_source, NAME)\n",
    "engine.run(common.batch_generator(buffer, params.replay_initial,\n",
    "                                  params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88e8c4-1134-4eb5-9fc4-8fd1b7caeec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
