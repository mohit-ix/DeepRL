{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23f72ae-0ef1-4553-8950-f3933c34335d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import common, dqn_extra\n",
    "\n",
    "NAME = \"08_rainbow\"\n",
    "N_STEPS = 4\n",
    "PRIO_REPLAY_ALPHA = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a11f1be-326a-4570-a232-cbb21491eaae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_rainbow(batch, batch_weights, net, tgt_net, gamma,\n",
    "                      device=\"cpu\", double=True):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_values = net(states_v).gather(1, actions_v)\n",
    "    state_action_values = state_action_values.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        if double:\n",
    "            next_state_actions = net(next_states_v).max(1)[1]\n",
    "            next_state_actions = next_state_actions.unsqueeze(-1)\n",
    "            next_state_values = tgt_net(next_states_v).gather(\n",
    "                1, next_state_actions).squeeze(-1)\n",
    "        else:\n",
    "            next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        expected_state_action_values = \\\n",
    "            next_state_values.detach() * gamma + rewards_v\n",
    "    losses_v = (state_action_values -\n",
    "                expected_state_action_values) ** 2\n",
    "    losses_v *= batch_weights_v\n",
    "    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c915fb-c4a6-4501-bcf0-3890c100c3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_prio(batch, batch_weights, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "    losses_v = batch_weights_v * (state_action_values - expected_state_action_values) ** 2\n",
    "    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402c526f-1c14-494d-925d-af28379a4b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 151010689]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3060fb-80be-4ebe-9d2d-96fd2a2afff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = dqn_extra.RainbowDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "agent = ptan.agent.DQNAgent(net, selector, device = device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma, steps_count=N_STEPS)\n",
    "buffer = dqn_extra.PrioReplayBuffer(\n",
    "    exp_source, params.replay_size, PRIO_REPLAY_ALPHA)\n",
    "optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a8a98f-d307-46fe-ad85-ba360c258dd8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=819, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 2: reward=-21, steps=881, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 3: reward=-21, steps=880, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 4: reward=-21, steps=817, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 5: reward=-21, steps=822, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 6: reward=-21, steps=881, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 7: reward=-21, steps=821, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 8: reward=-21, steps=881, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 9: reward=-21, steps=816, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 10: reward=-21, steps=823, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 11: reward=-21, steps=878, speed=0.0 f/s, elapsed=0:01:08\n",
      "Episode 12: reward=-21, steps=910, speed=38.3 f/s, elapsed=0:01:14\n",
      "Episode 13: reward=-21, steps=819, speed=38.4 f/s, elapsed=0:01:35\n",
      "Episode 14: reward=-20, steps=893, speed=38.4 f/s, elapsed=0:01:58\n",
      "Episode 15: reward=-21, steps=776, speed=38.4 f/s, elapsed=0:02:18\n",
      "Episode 16: reward=-21, steps=757, speed=38.4 f/s, elapsed=0:02:37\n",
      "Episode 17: reward=-21, steps=759, speed=38.4 f/s, elapsed=0:02:56\n",
      "Episode 18: reward=-21, steps=755, speed=38.4 f/s, elapsed=0:03:16\n",
      "Episode 19: reward=-21, steps=821, speed=38.4 f/s, elapsed=0:03:37\n",
      "Episode 20: reward=-20, steps=1325, speed=38.4 f/s, elapsed=0:04:12\n",
      "Episode 21: reward=-21, steps=1059, speed=38.4 f/s, elapsed=0:04:39\n",
      "Episode 22: reward=-20, steps=911, speed=38.4 f/s, elapsed=0:05:04\n",
      "Episode 23: reward=-21, steps=779, speed=38.3 f/s, elapsed=0:05:26\n",
      "Episode 24: reward=-21, steps=788, speed=38.3 f/s, elapsed=0:05:47\n",
      "Episode 25: reward=-21, steps=776, speed=38.3 f/s, elapsed=0:06:09\n",
      "Episode 26: reward=-21, steps=785, speed=38.2 f/s, elapsed=0:06:30\n",
      "Episode 27: reward=-21, steps=775, speed=38.2 f/s, elapsed=0:06:52\n",
      "Episode 28: reward=-21, steps=823, speed=38.1 f/s, elapsed=0:07:15\n",
      "Episode 29: reward=-19, steps=960, speed=38.1 f/s, elapsed=0:07:42\n",
      "Episode 30: reward=-18, steps=1304, speed=38.0 f/s, elapsed=0:08:18\n",
      "Episode 31: reward=-21, steps=1093, speed=38.0 f/s, elapsed=0:08:49\n",
      "Episode 32: reward=-21, steps=1234, speed=37.9 f/s, elapsed=0:09:24\n",
      "Episode 33: reward=-20, steps=1192, speed=37.9 f/s, elapsed=0:09:57\n",
      "Episode 34: reward=-19, steps=1190, speed=37.8 f/s, elapsed=0:10:30\n",
      "Episode 35: reward=-21, steps=1025, speed=37.8 f/s, elapsed=0:10:59\n",
      "Episode 36: reward=-20, steps=1453, speed=37.8 f/s, elapsed=0:11:40\n",
      "Episode 37: reward=-17, steps=1864, speed=37.7 f/s, elapsed=0:12:33\n",
      "Episode 38: reward=-19, steps=1596, speed=37.7 f/s, elapsed=0:13:18\n",
      "Episode 39: reward=-18, steps=2279, speed=37.7 f/s, elapsed=0:14:18\n",
      "Episode 40: reward=-17, steps=2536, speed=37.7 f/s, elapsed=0:15:24\n",
      "Episode 41: reward=-11, steps=2801, speed=37.6 f/s, elapsed=0:16:43\n",
      "Episode 42: reward=-13, steps=2693, speed=37.6 f/s, elapsed=0:17:58\n",
      "Episode 43: reward=-18, steps=2894, speed=37.6 f/s, elapsed=0:19:19\n",
      "Episode 44: reward=-19, steps=2384, speed=37.5 f/s, elapsed=0:20:26\n",
      "Episode 45: reward=-18, steps=2396, speed=37.5 f/s, elapsed=0:21:33\n",
      "Episode 46: reward=-13, steps=3986, speed=37.5 f/s, elapsed=0:23:25\n",
      "Episode 47: reward=-18, steps=2809, speed=37.4 f/s, elapsed=0:24:45\n",
      "Episode 48: reward=-12, steps=4521, speed=37.4 f/s, elapsed=0:26:53\n",
      "Episode 49: reward=-16, steps=3540, speed=37.3 f/s, elapsed=0:28:34\n",
      "Episode 50: reward=-19, steps=3473, speed=37.3 f/s, elapsed=0:30:13\n",
      "Episode 51: reward=1, steps=5574, speed=37.2 f/s, elapsed=0:32:53\n",
      "Episode 52: reward=-17, steps=4217, speed=37.1 f/s, elapsed=0:35:04\n",
      "Episode 53: reward=-18, steps=2785, speed=37.0 f/s, elapsed=0:36:35\n",
      "Episode 54: reward=-14, steps=4770, speed=36.9 f/s, elapsed=0:39:08\n",
      "Episode 55: reward=-10, steps=4946, speed=36.8 f/s, elapsed=0:41:37\n",
      "Episode 56: reward=-14, steps=2995, speed=36.7 f/s, elapsed=0:43:07\n",
      "Episode 57: reward=-15, steps=3834, speed=36.6 f/s, elapsed=0:45:05\n",
      "Episode 58: reward=-14, steps=5004, speed=36.6 f/s, elapsed=0:47:37\n",
      "Episode 59: reward=-14, steps=5558, speed=36.5 f/s, elapsed=0:50:24\n",
      "Episode 60: reward=-15, steps=5338, speed=36.4 f/s, elapsed=0:53:05\n",
      "Episode 61: reward=-13, steps=4607, speed=36.4 f/s, elapsed=0:55:23\n",
      "Episode 62: reward=-8, steps=5237, speed=36.3 f/s, elapsed=0:58:01\n",
      "Episode 63: reward=-7, steps=5812, speed=36.3 f/s, elapsed=1:00:55\n",
      "Episode 64: reward=7, steps=5588, speed=36.2 f/s, elapsed=1:03:44\n",
      "Episode 65: reward=-6, steps=6806, speed=36.1 f/s, elapsed=1:07:08\n",
      "Episode 66: reward=-5, steps=6515, speed=36.1 f/s, elapsed=1:10:23\n",
      "Episode 67: reward=-3, steps=6631, speed=36.0 f/s, elapsed=1:13:42\n",
      "Episode 68: reward=4, steps=6313, speed=36.0 f/s, elapsed=1:16:52\n",
      "Episode 69: reward=-2, steps=6765, speed=35.9 f/s, elapsed=1:20:15\n",
      "Episode 70: reward=-3, steps=7550, speed=35.9 f/s, elapsed=1:24:02\n",
      "Episode 71: reward=-13, steps=3746, speed=35.8 f/s, elapsed=1:25:54\n",
      "Episode 72: reward=6, steps=5181, speed=35.8 f/s, elapsed=1:28:29\n",
      "Episode 73: reward=-12, steps=4215, speed=35.7 f/s, elapsed=1:30:36\n",
      "Episode 74: reward=-5, steps=7074, speed=35.7 f/s, elapsed=1:34:07\n",
      "Episode 75: reward=-16, steps=3447, speed=35.6 f/s, elapsed=1:35:49\n",
      "Episode 76: reward=-4, steps=5625, speed=35.6 f/s, elapsed=1:38:36\n",
      "Episode 77: reward=10, steps=4149, speed=35.5 f/s, elapsed=1:40:42\n",
      "Episode 78: reward=10, steps=4078, speed=35.4 f/s, elapsed=1:42:55\n",
      "Episode 79: reward=12, steps=4607, speed=35.3 f/s, elapsed=1:45:25\n",
      "Episode 80: reward=-6, steps=4819, speed=35.3 f/s, elapsed=1:48:02\n",
      "Episode 81: reward=-10, steps=4847, speed=35.2 f/s, elapsed=1:50:32\n",
      "Episode 82: reward=7, steps=5145, speed=35.1 f/s, elapsed=1:53:15\n",
      "Episode 83: reward=10, steps=5039, speed=35.1 f/s, elapsed=1:55:54\n",
      "Episode 84: reward=3, steps=3851, speed=35.0 f/s, elapsed=1:57:56\n",
      "Episode 85: reward=-5, steps=5989, speed=34.9 f/s, elapsed=2:01:03\n",
      "Episode 86: reward=9, steps=5419, speed=34.9 f/s, elapsed=2:03:47\n",
      "Episode 87: reward=7, steps=4969, speed=34.8 f/s, elapsed=2:06:20\n",
      "Episode 88: reward=7, steps=3799, speed=34.8 f/s, elapsed=2:08:17\n",
      "Episode 89: reward=-3, steps=6348, speed=34.7 f/s, elapsed=2:11:32\n",
      "Episode 90: reward=10, steps=4105, speed=34.7 f/s, elapsed=2:13:38\n",
      "Episode 91: reward=6, steps=4893, speed=34.7 f/s, elapsed=2:16:08\n",
      "Episode 92: reward=14, steps=3982, speed=34.6 f/s, elapsed=2:18:08\n",
      "Episode 93: reward=17, steps=3264, speed=34.6 f/s, elapsed=2:19:45\n",
      "Episode 94: reward=1, steps=5624, speed=34.6 f/s, elapsed=2:22:31\n",
      "Episode 95: reward=15, steps=3345, speed=34.6 f/s, elapsed=2:24:10\n",
      "Episode 96: reward=-9, steps=4868, speed=34.6 f/s, elapsed=2:26:36\n",
      "Episode 97: reward=1, steps=5562, speed=34.5 f/s, elapsed=2:29:24\n",
      "Episode 98: reward=7, steps=4711, speed=34.5 f/s, elapsed=2:31:48\n",
      "Episode 99: reward=13, steps=3596, speed=34.5 f/s, elapsed=2:33:37\n",
      "Episode 100: reward=9, steps=4014, speed=34.4 f/s, elapsed=2:35:37\n",
      "Episode 101: reward=12, steps=5005, speed=34.4 f/s, elapsed=2:38:06\n",
      "Episode 102: reward=10, steps=3642, speed=34.4 f/s, elapsed=2:39:54\n",
      "Episode 103: reward=16, steps=3013, speed=34.4 f/s, elapsed=2:41:23\n",
      "Episode 104: reward=18, steps=2894, speed=34.4 f/s, elapsed=2:42:49\n",
      "Episode 105: reward=6, steps=3797, speed=34.4 f/s, elapsed=2:44:45\n",
      "Episode 106: reward=18, steps=2287, speed=34.3 f/s, elapsed=2:45:55\n",
      "Episode 107: reward=20, steps=2192, speed=34.3 f/s, elapsed=2:47:03\n",
      "Episode 108: reward=12, steps=3171, speed=34.2 f/s, elapsed=2:48:40\n",
      "Episode 109: reward=13, steps=3647, speed=34.2 f/s, elapsed=2:50:31\n",
      "Episode 110: reward=10, steps=4364, speed=34.2 f/s, elapsed=2:52:40\n",
      "Episode 111: reward=16, steps=2800, speed=34.2 f/s, elapsed=2:54:02\n",
      "Episode 112: reward=21, steps=2076, speed=34.2 f/s, elapsed=2:55:02\n",
      "Episode 113: reward=11, steps=3493, speed=34.2 f/s, elapsed=2:56:45\n",
      "Episode 114: reward=21, steps=2048, speed=34.2 f/s, elapsed=2:57:48\n",
      "Episode 115: reward=8, steps=4480, speed=34.2 f/s, elapsed=3:00:03\n",
      "Episode 116: reward=12, steps=3630, speed=34.1 f/s, elapsed=3:01:52\n",
      "Episode 117: reward=19, steps=2579, speed=34.1 f/s, elapsed=3:03:09\n",
      "Episode 118: reward=19, steps=2367, speed=34.1 f/s, elapsed=3:04:20\n",
      "Episode 119: reward=8, steps=4636, speed=34.1 f/s, elapsed=3:06:38\n",
      "Episode 120: reward=21, steps=1988, speed=34.1 f/s, elapsed=3:07:38\n",
      "Episode 121: reward=21, steps=2112, speed=34.1 f/s, elapsed=3:08:41\n",
      "Episode 122: reward=21, steps=1993, speed=34.1 f/s, elapsed=3:09:40\n",
      "Episode 123: reward=18, steps=2519, speed=34.1 f/s, elapsed=3:10:55\n",
      "Episode 124: reward=21, steps=1993, speed=34.1 f/s, elapsed=3:11:54\n",
      "Episode 125: reward=21, steps=1993, speed=34.1 f/s, elapsed=3:12:52\n",
      "Episode 126: reward=21, steps=1988, speed=34.1 f/s, elapsed=3:13:51\n",
      "Episode 127: reward=21, steps=1988, speed=34.0 f/s, elapsed=3:14:50\n",
      "Episode 128: reward=21, steps=2049, speed=34.0 f/s, elapsed=3:15:50\n",
      "Episode 129: reward=15, steps=2814, speed=34.0 f/s, elapsed=3:17:13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21280\\2232913429.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_ignite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m engine.run(common.batch_generator(buffer, params.replay_initial,\n\u001b[1;32m---> 20\u001b[1;33m                                   params.batch_size))\n\u001b[0m",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                 \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                 \u001b[1;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_event_name\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDATALOADER_STOP_ITERATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_BATCH_STARTED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET_BATCH_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                     \u001b[0miter_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIProjects\\DeepRL\\Unit 8\\lib\\common.py\u001b[0m in \u001b[0;36mbatch_generator\u001b[1;34m(buffer, initial, batch_size)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopulate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIProjects\\DeepRL\\Unit 8\\lib\\dqn_extra.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprios\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_alpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         indices = np.random.choice(len(self.buffer),\n\u001b[0;32m    171\u001b[0m                                    batch_size, p=probs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_batch(engine, batch_data):\n",
    "    batch, batch_indices, batch_weights = batch_data\n",
    "    optimizer.zero_grad()\n",
    "    loss_v, sample_prios = calc_loss_prio(\n",
    "        batch, batch_weights, net, tgt_net.target_model,\n",
    "        gamma=params.gamma**N_STEPS, device=device)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    buffer.update_priorities(batch_indices, sample_prios)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"beta\": buffer.update_beta(engine.state.iteration),\n",
    "    }\n",
    "\n",
    "engine = Engine(process_batch)\n",
    "common.setup_ignite(engine, params, exp_source, NAME)\n",
    "engine.run(common.batch_generator(buffer, params.replay_initial,\n",
    "                                  params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c580f3b-bfdf-41fb-a089-4770764966ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
