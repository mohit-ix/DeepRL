{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0312002-41d4-4dd3-9833-305ae56e2d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "NAME = \"03_double\"\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_FRAME = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a27c09-79b2-4037-ae43-a63a250c03ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_double_dqn(batch, net, tgt_net, gamma,\n",
    "                         device=\"cpu\", double=True):\n",
    "    states, actions, rewards, dones, next_states = \\\n",
    "        common.unpack_batch(batch)\n",
    "    \n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    \n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v.type(torch.int64))\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        if double:\n",
    "            next_state_acts = net(next_states_v).max(1)[1]\n",
    "            next_state_acts = next_state_acts.unsqueeze(-1)\n",
    "            next_state_vals = tgt_net(next_states_v).gather(\n",
    "                1, next_state_acts.type(torch.int64)).squeeze(-1)\n",
    "        else:\n",
    "            next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "        exp_sa_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, exp_sa_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32488d7e-9568-4c4d-99b6-30b8566132c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 151010689]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bc9314d-a7ee-4719-9c04-3679936d1dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device = device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=params.replay_size)\n",
    "optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "336ae906-85f7-427c-9e52-1839e1d60c74",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=818, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 2: reward=-19, steps=1022, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 3: reward=-21, steps=850, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 4: reward=-21, steps=840, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 5: reward=-20, steps=895, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 6: reward=-19, steps=958, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 7: reward=-20, steps=992, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 8: reward=-21, steps=819, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 9: reward=-21, steps=783, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 10: reward=-21, steps=937, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 11: reward=-19, steps=1038, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 12: reward=-20, steps=972, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 13: reward=-21, steps=895, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 14: reward=-19, steps=924, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 15: reward=-19, steps=1034, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 16: reward=-21, steps=917, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 17: reward=-19, steps=1133, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 18: reward=-17, steps=1280, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 19: reward=-20, steps=957, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 20: reward=-21, steps=902, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 21: reward=-21, steps=1016, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 22: reward=-20, steps=1068, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 23: reward=-21, steps=876, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 24: reward=-21, steps=846, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 25: reward=-20, steps=835, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 26: reward=-20, steps=864, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 27: reward=-21, steps=860, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 28: reward=-20, steps=895, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 29: reward=-18, steps=1053, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 30: reward=-20, steps=1065, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 31: reward=-21, steps=849, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 32: reward=-21, steps=879, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 33: reward=-21, steps=758, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 34: reward=-18, steps=1074, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 35: reward=-20, steps=915, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 36: reward=-19, steps=1048, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 37: reward=-21, steps=873, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 38: reward=-19, steps=1034, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 39: reward=-20, steps=917, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 40: reward=-20, steps=1030, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 41: reward=-21, steps=939, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 42: reward=-19, steps=1036, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 43: reward=-21, steps=787, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 44: reward=-20, steps=1012, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 45: reward=-20, steps=863, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 46: reward=-21, steps=897, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 47: reward=-21, steps=759, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 48: reward=-21, steps=873, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 49: reward=-21, steps=836, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 50: reward=-21, steps=821, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 51: reward=-19, steps=1003, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 52: reward=-21, steps=900, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 53: reward=-19, steps=1021, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 54: reward=-20, steps=896, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 55: reward=-20, steps=859, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 56: reward=-20, steps=946, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 57: reward=-20, steps=1073, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 58: reward=-20, steps=894, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 59: reward=-21, steps=817, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 60: reward=-21, steps=758, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 61: reward=-21, steps=759, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 62: reward=-19, steps=970, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 63: reward=-19, steps=1094, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 64: reward=-21, steps=790, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 65: reward=-20, steps=890, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 66: reward=-20, steps=909, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 67: reward=-21, steps=756, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 68: reward=-20, steps=966, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 69: reward=-21, steps=894, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 70: reward=-20, steps=987, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 71: reward=-21, steps=846, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 72: reward=-21, steps=906, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 73: reward=-20, steps=887, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 74: reward=-21, steps=841, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 75: reward=-21, steps=783, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 76: reward=-21, steps=847, speed=0.0 f/s, elapsed=0:00:59\n",
      "Episode 77: reward=-20, steps=986, speed=63.7 f/s, elapsed=0:01:12\n",
      "Episode 78: reward=-21, steps=867, speed=63.8 f/s, elapsed=0:01:24\n",
      "Episode 79: reward=-20, steps=1136, speed=63.9 f/s, elapsed=0:01:41\n",
      "Episode 80: reward=-20, steps=898, speed=64.0 f/s, elapsed=0:01:54\n",
      "Episode 81: reward=-21, steps=851, speed=64.1 f/s, elapsed=0:02:07\n",
      "Episode 82: reward=-19, steps=998, speed=64.1 f/s, elapsed=0:02:22\n",
      "Episode 83: reward=-21, steps=940, speed=64.2 f/s, elapsed=0:02:35\n",
      "Episode 84: reward=-20, steps=935, speed=64.3 f/s, elapsed=0:02:49\n",
      "Episode 85: reward=-20, steps=999, speed=64.4 f/s, elapsed=0:03:04\n",
      "Episode 86: reward=-21, steps=876, speed=64.4 f/s, elapsed=0:03:17\n",
      "Episode 87: reward=-20, steps=960, speed=64.5 f/s, elapsed=0:03:31\n",
      "Episode 88: reward=-21, steps=966, speed=64.6 f/s, elapsed=0:03:45\n",
      "Episode 89: reward=-21, steps=936, speed=64.6 f/s, elapsed=0:03:59\n",
      "Episode 90: reward=-21, steps=998, speed=64.7 f/s, elapsed=0:04:14\n",
      "Episode 91: reward=-20, steps=890, speed=64.8 f/s, elapsed=0:04:27\n",
      "Episode 92: reward=-21, steps=906, speed=64.8 f/s, elapsed=0:04:40\n",
      "Episode 93: reward=-20, steps=981, speed=64.9 f/s, elapsed=0:04:55\n",
      "Episode 94: reward=-21, steps=879, speed=64.9 f/s, elapsed=0:05:07\n",
      "Episode 95: reward=-19, steps=961, speed=65.0 f/s, elapsed=0:05:21\n",
      "Episode 96: reward=-18, steps=1087, speed=65.1 f/s, elapsed=0:05:38\n",
      "Episode 97: reward=-20, steps=1016, speed=65.1 f/s, elapsed=0:05:52\n",
      "Episode 98: reward=-21, steps=842, speed=65.2 f/s, elapsed=0:06:05\n",
      "Episode 99: reward=-19, steps=1192, speed=65.2 f/s, elapsed=0:06:22\n",
      "Episode 100: reward=-20, steps=893, speed=65.3 f/s, elapsed=0:06:36\n",
      "Episode 101: reward=-20, steps=1002, speed=65.3 f/s, elapsed=0:06:51\n",
      "Episode 102: reward=-20, steps=1032, speed=65.4 f/s, elapsed=0:07:06\n",
      "Episode 103: reward=-21, steps=1001, speed=65.4 f/s, elapsed=0:07:20\n",
      "Episode 104: reward=-21, steps=780, speed=65.5 f/s, elapsed=0:07:32\n",
      "Episode 105: reward=-20, steps=881, speed=65.5 f/s, elapsed=0:07:45\n",
      "Episode 106: reward=-20, steps=832, speed=65.6 f/s, elapsed=0:07:57\n",
      "Episode 107: reward=-19, steps=1093, speed=65.6 f/s, elapsed=0:08:14\n",
      "Episode 108: reward=-21, steps=891, speed=65.5 f/s, elapsed=0:08:29\n",
      "Episode 109: reward=-20, steps=975, speed=65.3 f/s, elapsed=0:08:46\n",
      "Episode 110: reward=-19, steps=928, speed=65.1 f/s, elapsed=0:09:02\n",
      "Episode 111: reward=-21, steps=817, speed=65.2 f/s, elapsed=0:09:14\n",
      "Episode 112: reward=-20, steps=1068, speed=65.2 f/s, elapsed=0:09:30\n",
      "Episode 113: reward=-19, steps=1023, speed=65.1 f/s, elapsed=0:09:47\n",
      "Episode 114: reward=-17, steps=1375, speed=65.0 f/s, elapsed=0:10:10\n",
      "Episode 115: reward=-19, steps=1009, speed=65.0 f/s, elapsed=0:10:26\n",
      "Episode 116: reward=-18, steps=1126, speed=64.9 f/s, elapsed=0:10:44\n",
      "Episode 117: reward=-21, steps=881, speed=64.8 f/s, elapsed=0:10:59\n",
      "Episode 118: reward=-19, steps=1071, speed=64.7 f/s, elapsed=0:11:16\n",
      "Episode 119: reward=-18, steps=1325, speed=64.7 f/s, elapsed=0:11:37\n",
      "Episode 120: reward=-20, steps=1122, speed=64.7 f/s, elapsed=0:11:55\n",
      "Episode 121: reward=-19, steps=1230, speed=64.7 f/s, elapsed=0:12:14\n",
      "Episode 122: reward=-18, steps=1196, speed=64.6 f/s, elapsed=0:12:34\n",
      "Episode 123: reward=-20, steps=1180, speed=64.5 f/s, elapsed=0:12:53\n",
      "Episode 124: reward=-20, steps=1358, speed=64.5 f/s, elapsed=0:13:15\n",
      "Episode 125: reward=-18, steps=1290, speed=64.4 f/s, elapsed=0:13:35\n",
      "Episode 126: reward=-18, steps=1413, speed=64.4 f/s, elapsed=0:13:57\n",
      "Episode 127: reward=-19, steps=1329, speed=64.4 f/s, elapsed=0:14:18\n",
      "Episode 128: reward=-17, steps=1178, speed=64.4 f/s, elapsed=0:14:37\n",
      "Episode 129: reward=-18, steps=1335, speed=64.4 f/s, elapsed=0:14:58\n",
      "Episode 130: reward=-17, steps=1497, speed=64.3 f/s, elapsed=0:15:23\n",
      "Episode 131: reward=-17, steps=1609, speed=64.3 f/s, elapsed=0:15:48\n",
      "Episode 132: reward=-20, steps=1101, speed=64.3 f/s, elapsed=0:16:05\n",
      "Episode 133: reward=-15, steps=1831, speed=64.3 f/s, elapsed=0:16:34\n",
      "Episode 134: reward=-17, steps=1790, speed=64.3 f/s, elapsed=0:17:02\n",
      "Episode 135: reward=-18, steps=1397, speed=64.2 f/s, elapsed=0:17:24\n",
      "Episode 136: reward=-18, steps=1309, speed=64.2 f/s, elapsed=0:17:45\n",
      "Episode 137: reward=-21, steps=1241, speed=64.2 f/s, elapsed=0:18:04\n",
      "Episode 138: reward=-18, steps=1304, speed=64.2 f/s, elapsed=0:18:25\n",
      "Episode 139: reward=-16, steps=1509, speed=64.2 f/s, elapsed=0:18:49\n",
      "Episode 140: reward=-18, steps=1496, speed=64.2 f/s, elapsed=0:19:12\n",
      "Episode 141: reward=-16, steps=1689, speed=64.1 f/s, elapsed=0:19:40\n",
      "Episode 142: reward=-19, steps=2223, speed=64.1 f/s, elapsed=0:20:16\n",
      "Episode 143: reward=-18, steps=1853, speed=64.1 f/s, elapsed=0:20:45\n",
      "Episode 144: reward=-19, steps=1524, speed=64.1 f/s, elapsed=0:21:08\n",
      "Episode 145: reward=-17, steps=1666, speed=64.1 f/s, elapsed=0:21:34\n",
      "Episode 146: reward=-18, steps=1693, speed=64.0 f/s, elapsed=0:22:02\n",
      "Episode 147: reward=-17, steps=1820, speed=64.0 f/s, elapsed=0:22:31\n",
      "Episode 148: reward=-16, steps=2326, speed=64.0 f/s, elapsed=0:23:08\n",
      "Episode 149: reward=-19, steps=1625, speed=63.9 f/s, elapsed=0:23:34\n",
      "Episode 150: reward=-16, steps=2303, speed=63.9 f/s, elapsed=0:24:11\n",
      "Episode 151: reward=-16, steps=1989, speed=63.9 f/s, elapsed=0:24:42\n",
      "Episode 152: reward=-16, steps=2317, speed=63.9 f/s, elapsed=0:25:19\n",
      "Episode 153: reward=-15, steps=2035, speed=63.9 f/s, elapsed=0:25:51\n",
      "Episode 154: reward=-14, steps=2584, speed=63.8 f/s, elapsed=0:26:33\n",
      "Episode 155: reward=-18, steps=2501, speed=63.8 f/s, elapsed=0:27:13\n",
      "Episode 156: reward=-21, steps=2104, speed=63.7 f/s, elapsed=0:27:47\n",
      "Episode 157: reward=-14, steps=2232, speed=63.7 f/s, elapsed=0:28:22\n",
      "Episode 158: reward=-17, steps=2089, speed=63.7 f/s, elapsed=0:28:55\n",
      "Episode 159: reward=-12, steps=2991, speed=63.7 f/s, elapsed=0:29:42\n",
      "Episode 160: reward=-14, steps=2633, speed=63.7 f/s, elapsed=0:30:23\n",
      "Episode 161: reward=-18, steps=2109, speed=63.7 f/s, elapsed=0:30:56\n",
      "Episode 162: reward=-13, steps=3145, speed=63.7 f/s, elapsed=0:31:46\n",
      "Episode 163: reward=-15, steps=2529, speed=63.7 f/s, elapsed=0:32:26\n",
      "Episode 164: reward=-19, steps=2573, speed=63.7 f/s, elapsed=0:33:06\n",
      "Episode 165: reward=-15, steps=2591, speed=63.7 f/s, elapsed=0:33:47\n",
      "Episode 166: reward=-17, steps=2360, speed=63.7 f/s, elapsed=0:34:24\n",
      "Episode 167: reward=-17, steps=3318, speed=63.7 f/s, elapsed=0:35:16\n",
      "Episode 168: reward=-16, steps=2618, speed=63.7 f/s, elapsed=0:35:57\n",
      "Episode 169: reward=-9, steps=3572, speed=63.7 f/s, elapsed=0:36:53\n",
      "Episode 170: reward=-16, steps=2676, speed=63.7 f/s, elapsed=0:37:35\n",
      "Episode 171: reward=-15, steps=3453, speed=63.7 f/s, elapsed=0:38:29\n",
      "Episode 172: reward=-19, steps=2866, speed=63.7 f/s, elapsed=0:39:14\n",
      "Episode 173: reward=-12, steps=3101, speed=63.7 f/s, elapsed=0:40:03\n",
      "Episode 174: reward=-14, steps=2454, speed=63.7 f/s, elapsed=0:40:42\n",
      "Episode 175: reward=-13, steps=3683, speed=63.7 f/s, elapsed=0:41:40\n",
      "Episode 176: reward=-14, steps=3051, speed=63.7 f/s, elapsed=0:42:27\n",
      "Episode 177: reward=-18, steps=2586, speed=63.7 f/s, elapsed=0:43:08\n",
      "Episode 178: reward=-11, steps=3115, speed=63.7 f/s, elapsed=0:43:57\n",
      "Episode 179: reward=-15, steps=3562, speed=63.7 f/s, elapsed=0:44:53\n",
      "Episode 180: reward=-12, steps=3176, speed=63.7 f/s, elapsed=0:45:43\n",
      "Episode 181: reward=-13, steps=3271, speed=63.7 f/s, elapsed=0:46:34\n",
      "Episode 182: reward=-8, steps=4022, speed=63.7 f/s, elapsed=0:47:37\n",
      "Episode 183: reward=-13, steps=2912, speed=63.7 f/s, elapsed=0:48:23\n",
      "Episode 184: reward=-15, steps=3310, speed=63.7 f/s, elapsed=0:49:15\n",
      "Episode 185: reward=-13, steps=3360, speed=63.7 f/s, elapsed=0:50:08\n",
      "Episode 186: reward=-4, steps=4145, speed=63.7 f/s, elapsed=0:51:13\n",
      "Episode 187: reward=-17, steps=2193, speed=63.7 f/s, elapsed=0:51:47\n",
      "Episode 188: reward=-8, steps=3523, speed=63.7 f/s, elapsed=0:52:42\n",
      "Episode 189: reward=-9, steps=3370, speed=63.7 f/s, elapsed=0:53:35\n",
      "Episode 190: reward=-12, steps=2436, speed=63.7 f/s, elapsed=0:54:13\n",
      "Episode 191: reward=-16, steps=2069, speed=63.7 f/s, elapsed=0:54:46\n",
      "Episode 192: reward=-15, steps=2136, speed=63.7 f/s, elapsed=0:55:19\n",
      "Episode 193: reward=-15, steps=2288, speed=63.7 f/s, elapsed=0:55:55\n",
      "Episode 194: reward=-8, steps=3348, speed=63.7 f/s, elapsed=0:56:48\n",
      "Episode 195: reward=-12, steps=2795, speed=63.7 f/s, elapsed=0:57:33\n",
      "Episode 196: reward=-12, steps=3343, speed=63.7 f/s, elapsed=0:58:25\n",
      "Episode 197: reward=-12, steps=3281, speed=63.7 f/s, elapsed=0:59:17\n",
      "Episode 198: reward=-9, steps=2868, speed=63.7 f/s, elapsed=1:00:03\n",
      "Episode 199: reward=-6, steps=3424, speed=63.7 f/s, elapsed=1:00:57\n",
      "Episode 200: reward=-3, steps=3755, speed=63.7 f/s, elapsed=1:01:56\n",
      "Episode 201: reward=-8, steps=3176, speed=63.7 f/s, elapsed=1:02:46\n",
      "Episode 202: reward=-6, steps=3757, speed=63.7 f/s, elapsed=1:03:45\n",
      "Episode 203: reward=-4, steps=4257, speed=63.7 f/s, elapsed=1:04:52\n",
      "Episode 204: reward=-15, steps=2200, speed=63.7 f/s, elapsed=1:05:26\n",
      "Episode 205: reward=-11, steps=3021, speed=63.7 f/s, elapsed=1:06:13\n",
      "Episode 206: reward=-11, steps=2900, speed=63.7 f/s, elapsed=1:06:59\n",
      "Episode 207: reward=-17, steps=2171, speed=63.6 f/s, elapsed=1:07:35\n",
      "Episode 208: reward=-11, steps=2789, speed=63.6 f/s, elapsed=1:08:19\n",
      "Episode 209: reward=-9, steps=2931, speed=63.6 f/s, elapsed=1:09:04\n",
      "Episode 210: reward=-11, steps=3205, speed=63.7 f/s, elapsed=1:09:52\n",
      "Episode 211: reward=-10, steps=3364, speed=63.8 f/s, elapsed=1:10:42\n",
      "Episode 212: reward=-14, steps=2401, speed=63.9 f/s, elapsed=1:11:17\n",
      "Episode 213: reward=-14, steps=2754, speed=63.9 f/s, elapsed=1:11:58\n",
      "Episode 214: reward=1, steps=3560, speed=64.0 f/s, elapsed=1:12:50\n",
      "Episode 215: reward=-6, steps=3374, speed=64.1 f/s, elapsed=1:13:40\n",
      "Episode 216: reward=-6, steps=3510, speed=64.2 f/s, elapsed=1:14:32\n",
      "Episode 217: reward=-6, steps=3446, speed=64.2 f/s, elapsed=1:15:23\n",
      "Episode 218: reward=-7, steps=3159, speed=64.3 f/s, elapsed=1:16:09\n",
      "Episode 219: reward=-2, steps=4305, speed=64.4 f/s, elapsed=1:17:13\n",
      "Episode 220: reward=2, steps=4327, speed=64.4 f/s, elapsed=1:18:17\n",
      "Episode 221: reward=-12, steps=2901, speed=64.5 f/s, elapsed=1:18:59\n",
      "Episode 222: reward=-4, steps=3768, speed=64.6 f/s, elapsed=1:19:55\n",
      "Episode 223: reward=-3, steps=3992, speed=64.6 f/s, elapsed=1:20:54\n",
      "Episode 224: reward=-11, steps=3188, speed=64.7 f/s, elapsed=1:21:41\n",
      "Episode 225: reward=4, steps=4206, speed=64.8 f/s, elapsed=1:22:43\n",
      "Episode 226: reward=-6, steps=4577, speed=64.8 f/s, elapsed=1:23:51\n",
      "Episode 227: reward=2, steps=4329, speed=64.9 f/s, elapsed=1:24:54\n",
      "Episode 228: reward=-8, steps=3291, speed=64.9 f/s, elapsed=1:25:43\n",
      "Episode 229: reward=-7, steps=3708, speed=65.0 f/s, elapsed=1:26:38\n",
      "Episode 230: reward=4, steps=4041, speed=65.1 f/s, elapsed=1:27:37\n",
      "Episode 231: reward=-5, steps=3903, speed=65.1 f/s, elapsed=1:28:34\n",
      "Episode 232: reward=2, steps=4366, speed=65.2 f/s, elapsed=1:29:39\n",
      "Episode 233: reward=-4, steps=4052, speed=65.2 f/s, elapsed=1:30:39\n",
      "Episode 234: reward=-4, steps=3680, speed=65.3 f/s, elapsed=1:31:33\n",
      "Episode 235: reward=-4, steps=4696, speed=65.3 f/s, elapsed=1:32:42\n",
      "Episode 236: reward=1, steps=4006, speed=65.4 f/s, elapsed=1:33:42\n",
      "Episode 237: reward=7, steps=4079, speed=65.4 f/s, elapsed=1:34:42\n",
      "Episode 238: reward=9, steps=4419, speed=65.5 f/s, elapsed=1:35:47\n",
      "Episode 239: reward=-8, steps=3248, speed=65.5 f/s, elapsed=1:36:35\n",
      "Episode 240: reward=2, steps=4646, speed=65.5 f/s, elapsed=1:37:44\n",
      "Episode 241: reward=2, steps=3811, speed=65.6 f/s, elapsed=1:38:40\n",
      "Episode 242: reward=-1, steps=3933, speed=65.6 f/s, elapsed=1:39:38\n",
      "Episode 243: reward=9, steps=3872, speed=65.7 f/s, elapsed=1:40:35\n",
      "Episode 244: reward=8, steps=3379, speed=65.7 f/s, elapsed=1:41:25\n",
      "Episode 245: reward=3, steps=3503, speed=65.8 f/s, elapsed=1:42:17\n",
      "Episode 246: reward=3, steps=3529, speed=65.8 f/s, elapsed=1:43:10\n",
      "Episode 247: reward=5, steps=3837, speed=65.8 f/s, elapsed=1:44:06\n",
      "Episode 248: reward=9, steps=3152, speed=65.9 f/s, elapsed=1:44:53\n",
      "Episode 249: reward=5, steps=3543, speed=65.9 f/s, elapsed=1:45:45\n",
      "Episode 250: reward=5, steps=3700, speed=65.9 f/s, elapsed=1:46:41\n",
      "Episode 251: reward=10, steps=2780, speed=65.9 f/s, elapsed=1:47:22\n",
      "Episode 252: reward=13, steps=2866, speed=66.0 f/s, elapsed=1:48:04\n",
      "Episode 253: reward=6, steps=3496, speed=66.0 f/s, elapsed=1:48:55\n",
      "Episode 254: reward=15, steps=2699, speed=66.1 f/s, elapsed=1:49:35\n",
      "Episode 255: reward=10, steps=3109, speed=66.1 f/s, elapsed=1:50:21\n",
      "Episode 256: reward=6, steps=3364, speed=66.1 f/s, elapsed=1:51:10\n",
      "Episode 257: reward=-1, steps=3749, speed=66.2 f/s, elapsed=1:52:06\n",
      "Episode 258: reward=3, steps=3705, speed=66.2 f/s, elapsed=1:53:00\n",
      "Episode 259: reward=7, steps=3682, speed=66.2 f/s, elapsed=1:53:54\n",
      "Episode 260: reward=11, steps=2609, speed=66.3 f/s, elapsed=1:54:33\n",
      "Episode 261: reward=9, steps=3199, speed=66.3 f/s, elapsed=1:55:20\n",
      "Episode 262: reward=11, steps=3182, speed=66.3 f/s, elapsed=1:56:07\n",
      "Episode 263: reward=14, steps=2633, speed=66.4 f/s, elapsed=1:56:45\n",
      "Episode 264: reward=12, steps=2551, speed=66.4 f/s, elapsed=1:57:23\n",
      "Episode 265: reward=13, steps=2682, speed=66.5 f/s, elapsed=1:58:02\n",
      "Episode 266: reward=20, steps=1804, speed=66.5 f/s, elapsed=1:58:29\n",
      "Episode 267: reward=13, steps=2489, speed=66.5 f/s, elapsed=1:59:05\n",
      "Episode 268: reward=12, steps=2532, speed=66.5 f/s, elapsed=1:59:43\n",
      "Episode 269: reward=14, steps=2477, speed=66.6 f/s, elapsed=2:00:19\n",
      "Episode 270: reward=17, steps=2252, speed=66.6 f/s, elapsed=2:00:52\n",
      "Episode 271: reward=17, steps=2531, speed=66.6 f/s, elapsed=2:01:30\n",
      "Episode 272: reward=18, steps=1925, speed=66.6 f/s, elapsed=2:01:58\n",
      "Episode 273: reward=18, steps=2280, speed=66.6 f/s, elapsed=2:02:33\n",
      "Episode 274: reward=14, steps=2304, speed=66.5 f/s, elapsed=2:03:10\n",
      "Episode 275: reward=16, steps=2516, speed=66.5 f/s, elapsed=2:03:49\n",
      "Episode 276: reward=17, steps=2112, speed=66.4 f/s, elapsed=2:04:23\n",
      "Episode 277: reward=19, steps=1813, speed=66.4 f/s, elapsed=2:04:51\n",
      "Episode 278: reward=17, steps=2193, speed=66.3 f/s, elapsed=2:05:25\n",
      "Episode 279: reward=19, steps=1928, speed=66.3 f/s, elapsed=2:05:54\n",
      "Episode 280: reward=18, steps=1934, speed=66.3 f/s, elapsed=2:06:23\n",
      "Episode 281: reward=17, steps=2188, speed=66.3 f/s, elapsed=2:06:56\n",
      "Episode 282: reward=18, steps=2092, speed=66.4 f/s, elapsed=2:07:27\n",
      "Episode 283: reward=17, steps=1949, speed=66.4 f/s, elapsed=2:07:56\n",
      "Episode 284: reward=17, steps=2228, speed=66.4 f/s, elapsed=2:08:29\n",
      "Episode 285: reward=19, steps=1939, speed=66.4 f/s, elapsed=2:08:59\n",
      "Episode 286: reward=18, steps=1991, speed=66.4 f/s, elapsed=2:09:29\n",
      "Episode 287: reward=19, steps=1901, speed=66.4 f/s, elapsed=2:09:57\n",
      "Episode 288: reward=17, steps=2360, speed=66.4 f/s, elapsed=2:10:33\n",
      "Episode 289: reward=14, steps=2301, speed=66.4 f/s, elapsed=2:11:07\n",
      "Episode 290: reward=18, steps=1967, speed=66.4 f/s, elapsed=2:11:36\n",
      "Episode 291: reward=19, steps=1854, speed=66.4 f/s, elapsed=2:12:04\n",
      "Episode 292: reward=17, steps=2343, speed=66.4 f/s, elapsed=2:12:39\n",
      "Episode 293: reward=20, steps=1808, speed=66.5 f/s, elapsed=2:13:06\n",
      "Episode 294: reward=17, steps=2190, speed=66.5 f/s, elapsed=2:13:38\n",
      "Episode 295: reward=19, steps=1960, speed=66.5 f/s, elapsed=2:14:07\n",
      "Episode 296: reward=16, steps=2168, speed=66.5 f/s, elapsed=2:14:40\n",
      "Episode 297: reward=18, steps=2223, speed=66.5 f/s, elapsed=2:15:13\n",
      "Episode 298: reward=19, steps=1847, speed=66.5 f/s, elapsed=2:15:40\n",
      "Episode 299: reward=21, steps=1629, speed=66.5 f/s, elapsed=2:16:05\n",
      "Episode 300: reward=20, steps=1843, speed=66.6 f/s, elapsed=2:16:32\n",
      "Episode 301: reward=18, steps=1941, speed=66.6 f/s, elapsed=2:17:01\n",
      "Episode 302: reward=16, steps=2024, speed=66.6 f/s, elapsed=2:17:31\n",
      "Episode 303: reward=20, steps=1920, speed=66.6 f/s, elapsed=2:18:00\n",
      "Episode 304: reward=16, steps=2229, speed=66.6 f/s, elapsed=2:18:33\n",
      "Episode 305: reward=20, steps=1654, speed=66.6 f/s, elapsed=2:18:58\n",
      "Episode 306: reward=20, steps=1729, speed=66.6 f/s, elapsed=2:19:24\n",
      "Episode 307: reward=19, steps=2008, speed=66.6 f/s, elapsed=2:19:53\n",
      "Episode 308: reward=14, steps=2475, speed=66.6 f/s, elapsed=2:20:30\n",
      "Episode 309: reward=15, steps=2350, speed=66.6 f/s, elapsed=2:21:05\n",
      "Episode 310: reward=17, steps=2053, speed=66.6 f/s, elapsed=2:21:36\n",
      "Episode 311: reward=19, steps=1997, speed=66.7 f/s, elapsed=2:22:06\n",
      "Episode 312: reward=16, steps=2097, speed=66.7 f/s, elapsed=2:22:37\n",
      "Episode 313: reward=20, steps=1788, speed=66.7 f/s, elapsed=2:23:04\n",
      "Episode 314: reward=18, steps=1908, speed=66.7 f/s, elapsed=2:23:32\n",
      "Episode 315: reward=19, steps=1713, speed=66.7 f/s, elapsed=2:23:58\n",
      "Episode 316: reward=17, steps=2010, speed=66.7 f/s, elapsed=2:24:28\n",
      "Episode 317: reward=19, steps=1893, speed=66.7 f/s, elapsed=2:24:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7724\\458582408.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEngine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_ignite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf\"{NAME}={True}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'values'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                 \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                 \u001b[1;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7724\\458582408.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[1;34m(engine, batch)\u001b[0m\n\u001b[0;32m      3\u001b[0m     loss_v = calc_loss_double_dqn(batch, net, tgt_net.target_model,\n\u001b[0;32m      4\u001b[0m                                   \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                   double=True)\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mloss_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7724\\3093430217.py\u001b[0m in \u001b[0;36mcalc_loss_double_dqn\u001b[1;34m(batch, net, tgt_net, gamma, device, double)\u001b[0m\n\u001b[0;32m      2\u001b[0m                          device=\"cpu\", double=True):\n\u001b[0;32m      3\u001b[0m     \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstates_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\AIProjects\\DeepRL\\Unit 8\\lib\\common.py\u001b[0m in \u001b[0;36munpack_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mexp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ptan\\common\\wrappers.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = calc_loss_double_dqn(batch, net, tgt_net.target_model,\n",
    "                                  gamma=params.gamma, device=device,\n",
    "                                  double=True)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    epsilon_tracker.frame(engine.state.iteration)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    if engine.state.iteration % EVAL_EVERY_FRAME == 0:\n",
    "        eval_states = getattr(engine.state, \"eval_states\", None)\n",
    "        if eval_states is None:\n",
    "            eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "            eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "            eval_states = np.array(eval_states, copy=False)\n",
    "            engine.state.eval_states = eval_states\n",
    "        engine.state.metrics[\"values\"] = \\\n",
    "            common.calc_values_of_states(eval_states, net, device)\n",
    "    return {\n",
    "        \"loss\" : loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }\n",
    "\n",
    "engine = Engine(process_batch)\n",
    "common.setup_ignite(engine, params, exp_source, f\"{NAME}={True}\", extra_metrics=('values',))\n",
    "engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b32dc0-43b3-4d85-8d16-80a13e16fc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
