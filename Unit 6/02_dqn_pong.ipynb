{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c6f416-9400-49a0-a79d-d0f1311dea9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For using wrapper functions by OpenAI Baseline\n",
    "from lib import wrappers\n",
    "# For importing neural network for dqn model\n",
    "from lib import dqn_model\n",
    "\n",
    "# keeping a record of time\n",
    "import time \n",
    "# for using arrays\n",
    "import numpy as np\n",
    "# for creating named tuples\n",
    "import collections\n",
    "\n",
    "#import gym\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53c99810-7c75-4923-9ff4-8bcfc49ca8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Default Environment\n",
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "# Max mean reward to reach\n",
    "MEAN_REWARD_BOUND = 21\n",
    "\n",
    "# gamma value for discount\n",
    "GAMMA = 0.9\n",
    "# batch size sample for replay buffer\n",
    "BATCH_SIZE = 32 \n",
    "# max replay buffer size\n",
    "REPLAY_SIZE = 10000\n",
    "# learning rate\n",
    "LEARNING_RATE = 1e-5\n",
    "# how frequent we link model weights\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "# steps after which replay buffer start populating\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "# no. of frames for the random actions\n",
    "EPSILON_DECAY_LAST_FRAME = 175000\n",
    "# totally random values\n",
    "EPSILON_START = 1.0\n",
    "# 1% random values\n",
    "EPSILON_FINAL = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b29ab6-3f21-4311-bd87-07a9bed35b24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a named tuple for Experiences\n",
    "Experience = collections.namedtuple(\n",
    "    'Experience', field_names=['state', 'action', 'reward',\n",
    "                              'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    '''\n",
    "    Responsible for creating a replay buffer\n",
    "    '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "        \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size,\n",
    "                                   replace=False)\n",
    "        states, actions, rewards, dones, next_states = \\\n",
    "            zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), \\\n",
    "               np.array(rewards, dtype = np.float32), \\\n",
    "               np.array(dones, dtype = np.uint8), \\\n",
    "               np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6673d015-9a95-4ce5-8615-0222f5f26a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward,\n",
    "                         is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061c1267-6149-4649-8c91-34d2c55afa88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(np.array(\n",
    "        states, copy=False)).to(device)\n",
    "    next_states_v = torch.tensor(np.array(\n",
    "        next_states, copy=False)).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(\n",
    "        1, actions_v.type(torch.int64).unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + \\\n",
    "                                   rewards_v\n",
    "    return nn.MSELoss()(state_action_values,\n",
    "                        expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b63b187-f662-4596-b9bd-1a93692fc9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=18, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = wrappers.make_env(\"PongNoFrameskip-v4\")\n",
    "#env = gym.wrappers.Monitor(env, director=\"PongNoFrameskip-v4\", force=True)\n",
    "\n",
    "net = dqn_model.DQN(env.observation_space.shape,\n",
    "                    env.action_space.n).to(device)\n",
    "tgt_net = dqn_model.DQN(env.observation_space.shape,\n",
    "                        env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-PongNoFrameskip-v4\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a37c067-a083-4a2c-ad1b-16d914200171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_m_reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bab28e-c4e2-45b6-bc44-aba466e3d1cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START -\n",
    "                  frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        m_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, reward %.3f, \"\n",
    "              \"eps %.2f, speed %.2f f/s\" % (\n",
    "            frame_idx, len(total_rewards), m_reward, epsilon,\n",
    "            speed\n",
    "        ))\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if best_m_reward is None or best_m_reward < m_reward:\n",
    "            torch.save(net.state_dict(), os.path.join(\n",
    "                \"PongNoFrameskip_state\", (\"PongNoFrameskip-v4-best_%.0f.dat\" % m_reward)))\n",
    "            if best_m_reward is not None:\n",
    "                print(\"Best reward updated %.3f -> %.3f\" % (\n",
    "                    best_m_reward, m_reward))\n",
    "            best_m_reward = m_reward\n",
    "        if m_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68363d07-0a5f-4ef2-8148-9104565eb2f2",
   "metadata": {},
   "source": [
    "For Pong: The agent is trained for for 960k steps. The goal was to achieve the mean score of 21 steps. It reached mean score of 19 steps at 650k steps. After 650k steps, the mean reward. \n",
    "The result of the training: https://youtu.be/03Pl5Odc2jM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c1769-32de-493c-8979-9619f41b7b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
