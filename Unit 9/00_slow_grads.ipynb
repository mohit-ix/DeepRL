{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd1533e-af65-4372-9e4f-a09131638948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "NAME = \"00_slow_grads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be3a387-6da3-4f36-b72d-62c046c5db9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)\n",
    "\n",
    "\n",
    "class DQNAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    DQNAgent is a memoryless DQN agent which calculates Q values\n",
    "    from the observations and  converts them into the actions using action_selector\n",
    "    \"\"\"\n",
    "    def __init__(self, dqn_model, action_selector, device=\"cpu\", preprocessor=ptan.agent.default_states_preprocessor):\n",
    "        self.dqn_model = dqn_model\n",
    "        self.action_selector = action_selector\n",
    "        self.preprocessor = preprocessor\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, states, agent_states=None):\n",
    "        if agent_states is None:\n",
    "            agent_states = [None] * len(states)\n",
    "        if self.preprocessor is not None:\n",
    "            states = self.preprocessor(states)\n",
    "            if torch.is_tensor(states):\n",
    "                states = states.to(self.device)\n",
    "        q_v = self.dqn_model(states)\n",
    "        q = q_v.data.cpu().numpy()\n",
    "        actions = self.action_selector(q)\n",
    "        return actions, agent_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a80c687-a689-4986-9e99-1b23ebed5381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_dqn(batch, net, tgt_net, gamma, device=\"cpu\", cuda_async=False):\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device, non_blocking=cuda_async)\n",
    "    next_states_v = torch.tensor(next_states).to(device, non_blocking=cuda_async)\n",
    "    actions_v = torch.tensor(actions).to(device, non_blocking=cuda_async)\n",
    "    rewards_v = torch.tensor(rewards).to(device, non_blocking=cuda_async)\n",
    "    done_mask = torch.BoolTensor(dones).to(device, non_blocking=cuda_async)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "\n",
    "    expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8bf8fd-4efc-4131-9851-c27d560d51b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 151010689]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting rid of missing metrics warning\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(params.env_name)\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4977880d-b2bf-471a-8ca3-1adafacb5422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma, steps_count=1)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=params.replay_size)\n",
    "optimizer = optim.Adam(net.parameters(), lr = params.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7db1e02-096d-446a-a29d-7484c028e1c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                                  gamma=params.gamma, device=device)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    epsilon_tracker.frame(engine.state.iteration)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }\n",
    "\n",
    "engine=Engine(process_batch)\n",
    "ptan_ignite.EndOfEpisodeHandler(exp_source, bound_avg_reward=15.0).attach(engine)\n",
    "ptan_ignite.EpisodeFPSHandler().attach(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27720de-95ac-4883-b740-992cf6b22b72",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-19.0, steps=931, speed=0.000 frames/s, elapsed=0:02:56.770834\n",
      "Episode 2: reward=-21.0, steps=789, speed=0.000 frames/s, elapsed=0:02:56.771836\n",
      "Episode 3: reward=-18.0, steps=1222, speed=0.000 frames/s, elapsed=0:02:56.771836\n",
      "Episode 4: reward=-21.0, steps=928, speed=0.000 frames/s, elapsed=0:02:56.771836\n",
      "Episode 5: reward=-21.0, steps=956, speed=0.000 frames/s, elapsed=0:02:56.772834\n",
      "Episode 6: reward=-21.0, steps=806, speed=0.000 frames/s, elapsed=0:02:56.772834\n",
      "Episode 7: reward=-20.0, steps=898, speed=0.000 frames/s, elapsed=0:02:56.772834\n",
      "Episode 8: reward=-20.0, steps=921, speed=0.000 frames/s, elapsed=0:02:56.773834\n",
      "Episode 9: reward=-21.0, steps=1025, speed=0.000 frames/s, elapsed=0:02:56.773834\n",
      "Episode 10: reward=-21.0, steps=1024, speed=0.000 frames/s, elapsed=0:02:56.773834\n",
      "Episode 11: reward=-20.0, steps=1060, speed=74.698 frames/s, elapsed=0:03:04.270605\n",
      "Episode 12: reward=-21.0, steps=808, speed=74.699 frames/s, elapsed=0:03:15.079424\n",
      "Episode 13: reward=-20.0, steps=1002, speed=74.705 frames/s, elapsed=0:03:28.445319\n",
      "Episode 14: reward=-19.0, steps=949, speed=74.718 frames/s, elapsed=0:03:41.036577\n",
      "Episode 15: reward=-21.0, steps=1039, speed=74.664 frames/s, elapsed=0:03:55.465658\n",
      "Episode 16: reward=-21.0, steps=821, speed=74.663 frames/s, elapsed=0:04:06.465284\n",
      "Episode 17: reward=-21.0, steps=994, speed=74.604 frames/s, elapsed=0:04:20.325076\n",
      "Episode 18: reward=-20.0, steps=967, speed=74.570 frames/s, elapsed=0:04:33.591929\n",
      "Episode 19: reward=-20.0, steps=1047, speed=74.495 frames/s, elapsed=0:04:48.371663\n",
      "Episode 20: reward=-21.0, steps=784, speed=74.457 frames/s, elapsed=0:04:59.172181\n",
      "Episode 21: reward=-21.0, steps=966, speed=74.419 frames/s, elapsed=0:05:12.485027\n",
      "Episode 22: reward=-20.0, steps=1103, speed=74.419 frames/s, elapsed=0:05:27.314781\n",
      "Episode 23: reward=-21.0, steps=760, speed=74.418 frames/s, elapsed=0:05:37.531905\n",
      "Episode 24: reward=-20.0, steps=953, speed=74.416 frames/s, elapsed=0:05:50.351109\n",
      "Episode 25: reward=-18.0, steps=1118, speed=74.409 frames/s, elapsed=0:06:05.448163\n",
      "Episode 26: reward=-20.0, steps=864, speed=74.413 frames/s, elapsed=0:06:17.026772\n",
      "Episode 27: reward=-21.0, steps=848, speed=74.417 frames/s, elapsed=0:06:28.392805\n",
      "Episode 28: reward=-20.0, steps=1004, speed=74.404 frames/s, elapsed=0:06:42.006268\n",
      "Episode 29: reward=-21.0, steps=759, speed=74.399 frames/s, elapsed=0:06:52.243880\n",
      "Episode 30: reward=-18.0, steps=1017, speed=74.401 frames/s, elapsed=0:07:05.892922\n",
      "Episode 31: reward=-21.0, steps=937, speed=74.392 frames/s, elapsed=0:07:18.563409\n",
      "Episode 32: reward=-21.0, steps=896, speed=74.355 frames/s, elapsed=0:07:30.911999\n",
      "Episode 33: reward=-20.0, steps=909, speed=74.184 frames/s, elapsed=0:07:44.727882\n",
      "Episode 34: reward=-21.0, steps=934, speed=74.037 frames/s, elapsed=0:07:58.698375\n",
      "Episode 35: reward=-18.0, steps=1187, speed=73.971 frames/s, elapsed=0:08:15.487256\n",
      "Episode 36: reward=-21.0, steps=815, speed=73.917 frames/s, elapsed=0:08:26.919902\n",
      "Episode 37: reward=-21.0, steps=849, speed=73.794 frames/s, elapsed=0:08:39.443473\n",
      "Episode 38: reward=-21.0, steps=897, speed=73.742 frames/s, elapsed=0:08:52.050485\n",
      "Episode 39: reward=-19.0, steps=963, speed=73.704 frames/s, elapsed=0:09:05.446847\n",
      "Episode 40: reward=-20.0, steps=997, speed=73.683 frames/s, elapsed=0:09:19.171618\n",
      "Episode 41: reward=-21.0, steps=788, speed=73.686 frames/s, elapsed=0:09:29.845984\n",
      "Episode 42: reward=-20.0, steps=992, speed=73.629 frames/s, elapsed=0:09:43.853423\n",
      "Episode 43: reward=-19.0, steps=1152, speed=73.512 frames/s, elapsed=0:10:00.840887\n",
      "Episode 44: reward=-21.0, steps=896, speed=73.434 frames/s, elapsed=0:10:13.717436\n",
      "Episode 45: reward=-19.0, steps=1020, speed=73.332 frames/s, elapsed=0:10:28.636907\n",
      "Episode 46: reward=-20.0, steps=1056, speed=73.199 frames/s, elapsed=0:10:44.484562\n",
      "Episode 47: reward=-20.0, steps=1056, speed=73.108 frames/s, elapsed=0:10:59.868013\n",
      "Episode 48: reward=-20.0, steps=834, speed=72.950 frames/s, elapsed=0:11:12.655386\n",
      "Episode 49: reward=-21.0, steps=997, speed=72.849 frames/s, elapsed=0:11:27.339607\n",
      "Episode 50: reward=-21.0, steps=880, speed=72.804 frames/s, elapsed=0:11:39.802943\n",
      "Episode 51: reward=-21.0, steps=784, speed=72.717 frames/s, elapsed=0:11:51.251430\n",
      "Episode 52: reward=-21.0, steps=841, speed=72.659 frames/s, elapsed=0:12:03.295645\n",
      "Episode 53: reward=-20.0, steps=930, speed=72.605 frames/s, elapsed=0:12:16.592321\n",
      "Episode 54: reward=-19.0, steps=962, speed=72.572 frames/s, elapsed=0:12:30.153299\n",
      "Episode 55: reward=-21.0, steps=881, speed=72.534 frames/s, elapsed=0:12:42.622419\n",
      "Episode 56: reward=-18.0, steps=1045, speed=72.499 frames/s, elapsed=0:12:57.382780\n",
      "Episode 57: reward=-20.0, steps=1010, speed=72.446 frames/s, elapsed=0:13:11.845897\n",
      "Episode 58: reward=-19.0, steps=992, speed=72.357 frames/s, elapsed=0:13:26.424090\n",
      "Episode 59: reward=-21.0, steps=924, speed=72.212 frames/s, elapsed=0:13:40.615979\n",
      "Episode 60: reward=-20.0, steps=835, speed=72.003 frames/s, elapsed=0:13:54.144206\n",
      "Episode 61: reward=-20.0, steps=866, speed=71.936 frames/s, elapsed=0:14:06.756095\n",
      "Episode 62: reward=-21.0, steps=779, speed=71.958 frames/s, elapsed=0:14:17.424522\n",
      "Episode 63: reward=-20.0, steps=1084, speed=71.912 frames/s, elapsed=0:14:32.976774\n",
      "Episode 64: reward=-21.0, steps=756, speed=71.921 frames/s, elapsed=0:14:43.426019\n",
      "Episode 65: reward=-21.0, steps=784, speed=71.920 frames/s, elapsed=0:14:54.339979\n",
      "Episode 66: reward=-21.0, steps=817, speed=71.887 frames/s, elapsed=0:15:05.958545\n",
      "Episode 67: reward=-20.0, steps=863, speed=71.851 frames/s, elapsed=0:15:18.273350\n",
      "Episode 68: reward=-21.0, steps=785, speed=71.822 frames/s, elapsed=0:15:29.429801\n",
      "Episode 69: reward=-21.0, steps=821, speed=71.760 frames/s, elapsed=0:15:41.375045\n",
      "Episode 70: reward=-19.0, steps=1040, speed=71.691 frames/s, elapsed=0:15:56.600412\n",
      "Episode 71: reward=-21.0, steps=904, speed=71.670 frames/s, elapsed=0:16:09.397398\n",
      "Episode 72: reward=-21.0, steps=818, speed=71.679 frames/s, elapsed=0:16:20.735138\n",
      "Episode 73: reward=-21.0, steps=817, speed=71.670 frames/s, elapsed=0:16:32.205985\n",
      "Episode 74: reward=-21.0, steps=849, speed=71.676 frames/s, elapsed=0:16:44.005206\n",
      "Episode 75: reward=-21.0, steps=815, speed=71.637 frames/s, elapsed=0:16:55.689000\n",
      "Episode 76: reward=-20.0, steps=1044, speed=71.586 frames/s, elapsed=0:17:10.800747\n",
      "Episode 77: reward=-20.0, steps=848, speed=71.544 frames/s, elapsed=0:17:23.003576\n",
      "Episode 78: reward=-21.0, steps=849, speed=71.537 frames/s, elapsed=0:17:34.935294\n",
      "Episode 79: reward=-19.0, steps=976, speed=71.508 frames/s, elapsed=0:17:48.860348\n",
      "Episode 80: reward=-20.0, steps=865, speed=71.526 frames/s, elapsed=0:18:00.802558\n",
      "Episode 81: reward=-21.0, steps=883, speed=71.460 frames/s, elapsed=0:18:13.743040\n",
      "Episode 82: reward=-20.0, steps=862, speed=71.486 frames/s, elapsed=0:18:25.594877\n",
      "Episode 83: reward=-21.0, steps=881, speed=71.524 frames/s, elapsed=0:18:37.602020\n",
      "Episode 84: reward=-21.0, steps=852, speed=71.529 frames/s, elapsed=0:18:49.466178\n",
      "Episode 85: reward=-19.0, steps=941, speed=71.475 frames/s, elapsed=0:19:03.141376\n",
      "Episode 86: reward=-21.0, steps=878, speed=71.481 frames/s, elapsed=0:19:15.373974\n",
      "Episode 87: reward=-21.0, steps=845, speed=71.524 frames/s, elapsed=0:19:26.853761\n",
      "Episode 88: reward=-20.0, steps=1017, speed=71.513 frames/s, elapsed=0:19:41.175873\n",
      "Episode 89: reward=-19.0, steps=932, speed=71.520 frames/s, elapsed=0:19:54.146509\n",
      "Episode 90: reward=-21.0, steps=858, speed=71.511 frames/s, elapsed=0:20:06.221387\n",
      "Episode 91: reward=-20.0, steps=971, speed=71.546 frames/s, elapsed=0:20:19.475448\n",
      "Episode 92: reward=-21.0, steps=807, speed=71.554 frames/s, elapsed=0:20:30.691462\n",
      "Episode 93: reward=-21.0, steps=778, speed=71.547 frames/s, elapsed=0:20:41.617278\n",
      "Episode 94: reward=-20.0, steps=896, speed=71.548 frames/s, elapsed=0:20:54.128141\n",
      "Episode 95: reward=-21.0, steps=877, speed=71.559 frames/s, elapsed=0:21:06.299746\n",
      "Episode 96: reward=-20.0, steps=927, speed=71.568 frames/s, elapsed=0:21:19.171569\n",
      "Episode 97: reward=-21.0, steps=756, speed=71.557 frames/s, elapsed=0:21:29.811179\n",
      "Episode 98: reward=-20.0, steps=882, speed=71.541 frames/s, elapsed=0:21:42.280477\n",
      "Episode 99: reward=-21.0, steps=816, speed=71.552 frames/s, elapsed=0:21:53.598577\n",
      "Episode 100: reward=-20.0, steps=894, speed=71.533 frames/s, elapsed=0:22:06.260260\n",
      "Episode 101: reward=-21.0, steps=784, speed=71.480 frames/s, elapsed=0:22:17.641760\n",
      "Episode 102: reward=-21.0, steps=818, speed=71.490 frames/s, elapsed=0:22:29.004473\n",
      "Episode 103: reward=-21.0, steps=789, speed=71.477 frames/s, elapsed=0:22:40.143293\n",
      "Episode 104: reward=-21.0, steps=815, speed=71.463 frames/s, elapsed=0:22:51.656480\n",
      "Episode 105: reward=-21.0, steps=903, speed=71.468 frames/s, elapsed=0:23:04.252840\n",
      "Episode 106: reward=-21.0, steps=762, speed=71.419 frames/s, elapsed=0:23:15.294598\n",
      "Episode 107: reward=-19.0, steps=909, speed=71.315 frames/s, elapsed=0:23:29.020189\n",
      "Episode 108: reward=-20.0, steps=836, speed=71.297 frames/s, elapsed=0:23:40.890786\n",
      "Episode 109: reward=-21.0, steps=935, speed=71.257 frames/s, elapsed=0:23:54.384572\n",
      "Episode 110: reward=-21.0, steps=882, speed=71.148 frames/s, elapsed=0:24:07.787601\n",
      "Episode 111: reward=-21.0, steps=880, speed=71.054 frames/s, elapsed=0:24:21.030813\n",
      "Episode 112: reward=-21.0, steps=819, speed=70.958 frames/s, elapsed=0:24:33.389731\n",
      "Episode 113: reward=-21.0, steps=837, speed=70.934 frames/s, elapsed=0:24:45.394787\n",
      "Episode 114: reward=-21.0, steps=816, speed=70.918 frames/s, elapsed=0:24:57.029154\n",
      "Episode 115: reward=-21.0, steps=761, speed=70.946 frames/s, elapsed=0:25:07.549917\n",
      "Episode 116: reward=-21.0, steps=822, speed=70.969 frames/s, elapsed=0:25:18.947920\n",
      "Episode 117: reward=-21.0, steps=816, speed=70.930 frames/s, elapsed=0:25:30.778788\n",
      "Episode 118: reward=-20.0, steps=834, speed=70.921 frames/s, elapsed=0:25:42.609723\n",
      "Episode 119: reward=-21.0, steps=879, speed=70.906 frames/s, elapsed=0:25:55.136428\n",
      "Episode 120: reward=-21.0, steps=761, speed=70.913 frames/s, elapsed=0:26:05.815342\n",
      "Episode 121: reward=-21.0, steps=820, speed=70.953 frames/s, elapsed=0:26:17.061572\n",
      "Episode 122: reward=-21.0, steps=820, speed=70.995 frames/s, elapsed=0:26:28.281832\n",
      "Episode 123: reward=-21.0, steps=759, speed=71.000 frames/s, elapsed=0:26:38.940769\n",
      "Episode 124: reward=-21.0, steps=822, speed=71.031 frames/s, elapsed=0:26:50.269450\n",
      "Episode 125: reward=-20.0, steps=834, speed=71.057 frames/s, elapsed=0:27:01.796427\n",
      "Episode 126: reward=-21.0, steps=762, speed=71.082 frames/s, elapsed=0:27:12.340705\n",
      "Episode 127: reward=-21.0, steps=760, speed=71.100 frames/s, elapsed=0:27:22.892492\n",
      "Episode 128: reward=-21.0, steps=787, speed=71.129 frames/s, elapsed=0:27:33.742789\n",
      "Episode 129: reward=-21.0, steps=761, speed=71.136 frames/s, elapsed=0:27:44.393351\n",
      "Episode 130: reward=-21.0, steps=761, speed=71.164 frames/s, elapsed=0:27:54.878270\n",
      "Episode 131: reward=-21.0, steps=758, speed=71.189 frames/s, elapsed=0:28:05.352219\n",
      "Episode 132: reward=-20.0, steps=837, speed=71.215 frames/s, elapsed=0:28:16.891662\n",
      "Episode 133: reward=-21.0, steps=815, speed=71.243 frames/s, elapsed=0:28:28.114088\n",
      "Episode 134: reward=-21.0, steps=756, speed=71.206 frames/s, elapsed=0:28:39.010644\n",
      "Episode 135: reward=-21.0, steps=759, speed=71.098 frames/s, elapsed=0:28:50.547379\n",
      "Episode 136: reward=-21.0, steps=879, speed=71.104 frames/s, elapsed=0:29:02.860523\n",
      "Episode 137: reward=-21.0, steps=786, speed=70.996 frames/s, elapsed=0:29:14.822644\n",
      "Episode 138: reward=-21.0, steps=820, speed=70.991 frames/s, elapsed=0:29:26.408784\n",
      "Episode 139: reward=-21.0, steps=822, speed=70.978 frames/s, elapsed=0:29:38.101708\n",
      "Episode 140: reward=-20.0, steps=835, speed=70.965 frames/s, elapsed=0:29:49.967674\n",
      "Episode 141: reward=-21.0, steps=761, speed=70.923 frames/s, elapsed=0:30:01.021757\n",
      "Episode 142: reward=-21.0, steps=817, speed=70.883 frames/s, elapsed=0:30:12.878441\n",
      "Episode 143: reward=-20.0, steps=897, speed=70.794 frames/s, elapsed=0:30:26.374793\n",
      "Episode 144: reward=-21.0, steps=783, speed=70.710 frames/s, elapsed=0:30:38.135356\n",
      "Episode 145: reward=-21.0, steps=840, speed=70.675 frames/s, elapsed=0:30:50.319553\n",
      "Episode 146: reward=-21.0, steps=761, speed=70.667 frames/s, elapsed=0:31:01.144128\n",
      "Episode 147: reward=-21.0, steps=820, speed=70.659 frames/s, elapsed=0:31:12.817399\n",
      "Episode 148: reward=-21.0, steps=846, speed=70.665 frames/s, elapsed=0:31:24.735489\n",
      "Episode 149: reward=-21.0, steps=760, speed=70.713 frames/s, elapsed=0:31:35.138820\n",
      "Episode 150: reward=-21.0, steps=823, speed=70.762 frames/s, elapsed=0:31:46.384584\n",
      "Episode 151: reward=-21.0, steps=758, speed=70.786 frames/s, elapsed=0:31:56.920497\n",
      "Episode 152: reward=-21.0, steps=813, speed=70.785 frames/s, elapsed=0:32:08.415059\n",
      "Episode 153: reward=-20.0, steps=892, speed=70.810 frames/s, elapsed=0:32:20.800452\n",
      "Episode 154: reward=-21.0, steps=758, speed=70.792 frames/s, elapsed=0:32:31.639525\n",
      "Episode 155: reward=-21.0, steps=756, speed=70.780 frames/s, elapsed=0:32:42.409415\n",
      "Episode 156: reward=-20.0, steps=889, speed=70.785 frames/s, elapsed=0:32:54.924147\n",
      "Episode 157: reward=-21.0, steps=789, speed=70.787 frames/s, elapsed=0:33:06.055870\n",
      "Episode 158: reward=-21.0, steps=820, speed=70.762 frames/s, elapsed=0:33:17.845689\n",
      "Episode 159: reward=-21.0, steps=841, speed=70.747 frames/s, elapsed=0:33:29.861251\n",
      "Episode 160: reward=-21.0, steps=774, speed=70.741 frames/s, elapsed=0:33:40.849924\n",
      "Episode 161: reward=-21.0, steps=938, speed=70.759 frames/s, elapsed=0:33:53.942289\n",
      "Episode 162: reward=-20.0, steps=831, speed=70.686 frames/s, elapsed=0:34:06.320255\n",
      "Episode 163: reward=-21.0, steps=808, speed=70.694 frames/s, elapsed=0:34:17.691059\n",
      "Episode 164: reward=-21.0, steps=819, speed=70.669 frames/s, elapsed=0:34:29.480472\n",
      "Episode 165: reward=-21.0, steps=761, speed=70.637 frames/s, elapsed=0:34:40.499688\n",
      "Episode 166: reward=-21.0, steps=762, speed=70.579 frames/s, elapsed=0:34:51.747098\n",
      "Episode 167: reward=-21.0, steps=756, speed=70.542 frames/s, elapsed=0:35:02.753474\n",
      "Episode 168: reward=-21.0, steps=758, speed=70.575 frames/s, elapsed=0:35:13.252076\n",
      "Episode 169: reward=-21.0, steps=790, speed=70.582 frames/s, elapsed=0:35:24.391146\n",
      "Episode 170: reward=-21.0, steps=847, speed=70.595 frames/s, elapsed=0:35:36.276289\n",
      "Episode 171: reward=-21.0, steps=756, speed=70.578 frames/s, elapsed=0:35:47.118655\n",
      "Episode 172: reward=-21.0, steps=787, speed=70.477 frames/s, elapsed=0:35:59.125798\n",
      "Episode 173: reward=-21.0, steps=758, speed=70.373 frames/s, elapsed=0:36:10.737025\n",
      "Episode 174: reward=-20.0, steps=835, speed=70.370 frames/s, elapsed=0:36:22.631213\n",
      "Episode 175: reward=-21.0, steps=824, speed=70.320 frames/s, elapsed=0:36:34.774127\n",
      "Episode 176: reward=-21.0, steps=758, speed=70.293 frames/s, elapsed=0:36:45.764510\n",
      "Episode 177: reward=-21.0, steps=789, speed=70.243 frames/s, elapsed=0:36:57.401033\n",
      "Episode 178: reward=-20.0, steps=833, speed=70.199 frames/s, elapsed=0:37:09.639719\n",
      "Episode 179: reward=-21.0, steps=788, speed=70.148 frames/s, elapsed=0:37:21.287025\n",
      "Episode 180: reward=-21.0, steps=867, speed=70.132 frames/s, elapsed=0:37:33.790144\n",
      "Episode 181: reward=-20.0, steps=832, speed=70.152 frames/s, elapsed=0:37:45.490883\n",
      "Episode 182: reward=-21.0, steps=821, speed=70.147 frames/s, elapsed=0:37:57.230070\n",
      "Episode 183: reward=-21.0, steps=879, speed=70.176 frames/s, elapsed=0:38:09.515142\n",
      "Episode 184: reward=-21.0, steps=787, speed=70.171 frames/s, elapsed=0:38:20.768237\n",
      "Episode 185: reward=-21.0, steps=757, speed=70.134 frames/s, elapsed=0:38:31.847818\n",
      "Episode 186: reward=-21.0, steps=755, speed=70.103 frames/s, elapsed=0:38:42.854922\n",
      "Episode 187: reward=-21.0, steps=851, speed=70.051 frames/s, elapsed=0:38:55.459026\n",
      "Episode 188: reward=-21.0, steps=761, speed=70.017 frames/s, elapsed=0:39:06.591841\n",
      "Episode 189: reward=-21.0, steps=820, speed=69.976 frames/s, elapsed=0:39:18.655847\n",
      "Episode 190: reward=-21.0, steps=786, speed=70.010 frames/s, elapsed=0:39:29.622566\n",
      "Episode 191: reward=-21.0, steps=757, speed=69.975 frames/s, elapsed=0:39:40.718967\n",
      "Episode 192: reward=-21.0, steps=757, speed=69.835 frames/s, elapsed=0:39:52.733490\n",
      "Episode 193: reward=-21.0, steps=759, speed=69.708 frames/s, elapsed=0:40:04.696067\n",
      "Episode 194: reward=-21.0, steps=850, speed=69.596 frames/s, elapsed=0:40:17.951613\n",
      "Episode 195: reward=-21.0, steps=806, speed=69.563 frames/s, elapsed=0:40:29.813062\n",
      "Episode 196: reward=-21.0, steps=817, speed=69.615 frames/s, elapsed=0:40:41.135036\n",
      "Episode 197: reward=-21.0, steps=758, speed=69.651 frames/s, elapsed=0:40:51.748708\n",
      "Episode 198: reward=-21.0, steps=822, speed=69.708 frames/s, elapsed=0:41:03.088246\n",
      "Episode 199: reward=-21.0, steps=807, speed=69.764 frames/s, elapsed=0:41:14.218863\n",
      "Episode 200: reward=-21.0, steps=758, speed=69.735 frames/s, elapsed=0:41:25.308519\n",
      "Episode 201: reward=-21.0, steps=762, speed=69.691 frames/s, elapsed=0:41:36.595504\n",
      "Episode 202: reward=-21.0, steps=757, speed=69.598 frames/s, elapsed=0:41:48.229597\n",
      "Episode 203: reward=-21.0, steps=760, speed=69.362 frames/s, elapsed=0:42:01.382297\n",
      "Episode 204: reward=-21.0, steps=759, speed=69.367 frames/s, elapsed=0:42:12.286533\n",
      "Episode 205: reward=-21.0, steps=790, speed=69.342 frames/s, elapsed=0:42:23.882015\n",
      "Episode 206: reward=-21.0, steps=787, speed=69.330 frames/s, elapsed=0:42:35.330342\n",
      "Episode 207: reward=-21.0, steps=757, speed=69.326 frames/s, elapsed=0:42:46.283144\n",
      "Episode 208: reward=-21.0, steps=756, speed=69.300 frames/s, elapsed=0:42:57.393156\n",
      "Episode 209: reward=-21.0, steps=758, speed=69.238 frames/s, elapsed=0:43:08.848455\n",
      "Episode 210: reward=-21.0, steps=757, speed=69.134 frames/s, elapsed=0:43:20.665177\n",
      "Episode 211: reward=-21.0, steps=755, speed=69.067 frames/s, elapsed=0:43:32.146382\n",
      "Episode 212: reward=-21.0, steps=762, speed=69.024 frames/s, elapsed=0:43:43.535148\n",
      "Episode 213: reward=-20.0, steps=957, speed=68.980 frames/s, elapsed=0:43:57.853303\n",
      "Episode 214: reward=-21.0, steps=756, speed=68.940 frames/s, elapsed=0:44:09.140435\n",
      "Episode 215: reward=-21.0, steps=784, speed=68.873 frames/s, elapsed=0:44:21.097803\n",
      "Episode 216: reward=-21.0, steps=758, speed=68.845 frames/s, elapsed=0:44:32.328921\n",
      "Episode 217: reward=-21.0, steps=777, speed=68.806 frames/s, elapsed=0:44:43.944967\n",
      "Episode 218: reward=-19.0, steps=971, speed=68.864 frames/s, elapsed=0:44:57.481323\n",
      "Episode 219: reward=-21.0, steps=757, speed=68.886 frames/s, elapsed=0:45:08.302776\n",
      "Episode 220: reward=-21.0, steps=757, speed=68.903 frames/s, elapsed=0:45:19.160045\n",
      "Episode 221: reward=-21.0, steps=760, speed=68.977 frames/s, elapsed=0:45:29.624232\n",
      "Episode 222: reward=-21.0, steps=755, speed=69.050 frames/s, elapsed=0:45:40.020563\n",
      "Episode 223: reward=-21.0, steps=786, speed=69.127 frames/s, elapsed=0:45:50.803863\n",
      "Episode 224: reward=-21.0, steps=875, speed=69.191 frames/s, elapsed=0:46:02.898502\n",
      "Episode 225: reward=-21.0, steps=784, speed=69.264 frames/s, elapsed=0:46:13.665729\n",
      "Episode 226: reward=-20.0, steps=833, speed=69.295 frames/s, elapsed=0:46:25.428209\n",
      "Episode 227: reward=-21.0, steps=815, speed=69.361 frames/s, elapsed=0:46:36.651599\n",
      "Episode 228: reward=-21.0, steps=758, speed=69.425 frames/s, elapsed=0:46:47.099664\n",
      "Episode 229: reward=-21.0, steps=760, speed=69.423 frames/s, elapsed=0:46:58.062295\n",
      "Episode 230: reward=-21.0, steps=757, speed=69.412 frames/s, elapsed=0:47:09.051559\n",
      "Episode 231: reward=-21.0, steps=821, speed=69.290 frames/s, elapsed=0:47:22.019732\n",
      "Episode 232: reward=-21.0, steps=821, speed=69.188 frames/s, elapsed=0:47:34.815658\n",
      "Episode 233: reward=-21.0, steps=757, speed=69.182 frames/s, elapsed=0:47:45.804596\n",
      "Episode 234: reward=-21.0, steps=762, speed=69.165 frames/s, elapsed=0:47:56.956811\n",
      "Episode 235: reward=-21.0, steps=790, speed=69.162 frames/s, elapsed=0:48:08.400379\n",
      "Episode 236: reward=-21.0, steps=761, speed=69.150 frames/s, elapsed=0:48:19.502670\n",
      "Episode 237: reward=-21.0, steps=819, speed=69.178 frames/s, elapsed=0:48:31.109246\n",
      "Episode 238: reward=-21.0, steps=759, speed=69.139 frames/s, elapsed=0:48:42.399543\n",
      "Episode 239: reward=-21.0, steps=774, speed=69.126 frames/s, elapsed=0:48:53.696907\n",
      "Episode 240: reward=-21.0, steps=755, speed=69.099 frames/s, elapsed=0:49:04.836649\n",
      "Episode 241: reward=-21.0, steps=761, speed=68.991 frames/s, elapsed=0:49:16.782138\n",
      "Episode 242: reward=-21.0, steps=776, speed=68.985 frames/s, elapsed=0:49:28.082234\n",
      "Episode 243: reward=-20.0, steps=836, speed=68.963 frames/s, elapsed=0:49:40.399971\n",
      "Episode 244: reward=-21.0, steps=789, speed=68.918 frames/s, elapsed=0:49:52.223661\n",
      "Episode 245: reward=-21.0, steps=786, speed=68.852 frames/s, elapsed=0:50:04.199321\n",
      "Episode 246: reward=-20.0, steps=917, speed=68.824 frames/s, elapsed=0:50:17.796533\n",
      "Episode 247: reward=-21.0, steps=755, speed=68.769 frames/s, elapsed=0:50:29.227207\n",
      "Episode 248: reward=-21.0, steps=819, speed=68.772 frames/s, elapsed=0:50:41.111090\n",
      "Episode 249: reward=-21.0, steps=759, speed=68.769 frames/s, elapsed=0:50:52.165460\n",
      "Episode 250: reward=-21.0, steps=762, speed=68.755 frames/s, elapsed=0:51:03.367212\n",
      "Episode 251: reward=-21.0, steps=761, speed=68.769 frames/s, elapsed=0:51:14.323447\n",
      "Episode 252: reward=-21.0, steps=755, speed=68.728 frames/s, elapsed=0:51:25.636974\n",
      "Episode 253: reward=-21.0, steps=758, speed=68.693 frames/s, elapsed=0:51:36.950492\n",
      "Episode 254: reward=-21.0, steps=758, speed=68.630 frames/s, elapsed=0:51:48.514832\n",
      "Episode 255: reward=-21.0, steps=787, speed=68.598 frames/s, elapsed=0:52:00.257075\n",
      "Episode 256: reward=-21.0, steps=760, speed=68.592 frames/s, elapsed=0:52:11.384430\n",
      "Episode 257: reward=-21.0, steps=821, speed=68.618 frames/s, elapsed=0:52:23.135379\n",
      "Episode 258: reward=-21.0, steps=784, speed=68.651 frames/s, elapsed=0:52:34.290606\n",
      "Episode 259: reward=-21.0, steps=755, speed=68.721 frames/s, elapsed=0:52:44.757594\n",
      "Episode 260: reward=-21.0, steps=822, speed=68.697 frames/s, elapsed=0:52:56.930895\n",
      "Episode 261: reward=-21.0, steps=761, speed=68.715 frames/s, elapsed=0:53:07.860915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11356\\3654009473.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_handler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mptan_ignite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPeriodEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERS_100_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                 \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                 \u001b[1;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11356\\3245619744.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[1;34m(engine, batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n\u001b[0;32m      4\u001b[0m                                   gamma=params.gamma, device=device)\n\u001b[0;32m      5\u001b[0m     \u001b[0mloss_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    190\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "def episode_completed(trainer: Engine):\n",
    "    print(\"Episode %d: reward=%s, steps=%s, speed=%.3f frames/s, elapsed=%s\" % (\n",
    "        trainer.state.episode, trainer.state.episode_reward,\n",
    "        trainer.state.episode_steps, trainer.state.metrics.get('avg_fps', 0),\n",
    "        timedelta(seconds=trainer.state.metrics.get('time_passed', 0))))\n",
    "    \n",
    "@engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "def game_solved(trainer: Engine):\n",
    "    print(\"Game solved in %s, after %d episodes and %d iterations!\" % (\n",
    "        timedelta(seconds=trainer.state.metrics['time_passed']),\n",
    "        trainer.state.episode, trainer.state.iteration))\n",
    "    trainer.should_terminate = True\n",
    "    \n",
    "logdir = f\"runs/{datetime.now().minute}-{params.run_name}-{NAME}\"\n",
    "tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "RunningAverage(output_transform=lambda v: v['loss']).attach(engine, \"avg_loss\")\n",
    "\n",
    "episode_handler = tb_logger.OutputHandler(tag='episodes', metric_names=['reward', 'steps', 'avg_reward'])\n",
    "tb.attach(engine, log_handler=episode_handler, event_name=ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "\n",
    "# writing to tensorboard every 100 iterations\n",
    "ptan_ignite.PeriodicEvents().attach(engine)\n",
    "handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['avg_loss', 'avg_fps'],\n",
    "                                  output_transform=lambda a: a)\n",
    "tb.attach(engine, log_handler=handler, event_name=ptan_ignite.PeriodEvents.ITERS_100_COMPLETED)\n",
    "\n",
    "engine.run(batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926912c-9ebb-4c9b-876a-13dfb1194994",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
