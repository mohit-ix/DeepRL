{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73d0fe47-933a-4f30-b8f4-1364c895e38d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ptan\n",
    "\n",
    "import ptan.ignite as ptan_ignite\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "from lib import dqn_model, common\n",
    "from lib import atari_wrappers\n",
    "\n",
    "\n",
    "NAME = \"04_wrappers_n_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e65a18-05b9-4957-bb67-fb2f02b1917f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int, steps: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(steps)\n",
    "        yield buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0972bce2-c8f2-46f0-8261-8bf02100f54e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#getting rid of missing metrics warning\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "envs = []\n",
    "n_envs = 3\n",
    "for _ in range(n_envs):\n",
    "    env = atari_wrappers.make_atari(params.env_name, skip_noop=True, skip_maxskip=True)\n",
    "    env = atari_wrappers.wrap_deepmind(env, pytorch_img=True, frame_stack=True, frame_stack_count=2)\n",
    "    env.seed(common.SEED)\n",
    "    envs.append(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ffec47-fd62-4d76-9a34-a3242dba3ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params.batch_size *= n_envs\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=params.replay_size)\n",
    "optimizer = optim.Adam(net.parameters(), lr = params.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4baf0ee1-47c4-40a9-ad06-e2102128578c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                                  gamma=params.gamma, device=device)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    epsilon_tracker.frame(engine.state.iteration * n_envs)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }\n",
    "\n",
    "engine=Engine(process_batch)\n",
    "ptan_ignite.EndOfEpisodeHandler(exp_source, bound_avg_reward=15.0).attach(engine)\n",
    "ptan_ignite.EpisodeFPSHandler(fps_mul=n_envs).attach(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc19161-f5ef-44ae-982c-f837e3994fb4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21.0, steps=850, speed=0.000 frames/s, elapsed=0:01:47.709634\n",
      "Episode 2: reward=-21.0, steps=822, speed=0.000 frames/s, elapsed=0:01:47.710234\n",
      "Episode 3: reward=-21.0, steps=841, speed=0.000 frames/s, elapsed=0:01:47.710234\n",
      "Episode 4: reward=-20.0, steps=976, speed=0.000 frames/s, elapsed=0:01:47.710234\n",
      "Episode 5: reward=-20.0, steps=946, speed=0.000 frames/s, elapsed=0:01:47.711218\n",
      "Episode 6: reward=-19.0, steps=1129, speed=0.000 frames/s, elapsed=0:01:47.711218\n",
      "Episode 7: reward=-21.0, steps=869, speed=0.000 frames/s, elapsed=0:01:47.711218\n",
      "Episode 8: reward=-21.0, steps=968, speed=0.000 frames/s, elapsed=0:01:47.712223\n",
      "Episode 9: reward=-19.0, steps=919, speed=0.000 frames/s, elapsed=0:01:47.712223\n",
      "Episode 10: reward=-21.0, steps=762, speed=0.000 frames/s, elapsed=0:01:47.712223\n",
      "Episode 11: reward=-21.0, steps=762, speed=0.000 frames/s, elapsed=0:01:47.712223\n",
      "Episode 12: reward=-21.0, steps=822, speed=110.519 frames/s, elapsed=0:01:53.738087\n",
      "Episode 13: reward=-19.0, steps=1023, speed=110.583 frames/s, elapsed=0:02:02.733400\n",
      "Episode 14: reward=-21.0, steps=823, speed=110.655 frames/s, elapsed=0:02:09.933825\n",
      "Episode 15: reward=-21.0, steps=872, speed=110.733 frames/s, elapsed=0:02:17.553531\n",
      "Episode 16: reward=-21.0, steps=824, speed=110.781 frames/s, elapsed=0:02:24.820036\n",
      "Episode 17: reward=-20.0, steps=1078, speed=110.824 frames/s, elapsed=0:02:34.384470\n",
      "Episode 18: reward=-20.0, steps=836, speed=110.876 frames/s, elapsed=0:02:41.735295\n",
      "Episode 19: reward=-21.0, steps=790, speed=110.881 frames/s, elapsed=0:02:48.862115\n",
      "Episode 20: reward=-18.0, steps=1241, speed=111.108 frames/s, elapsed=0:02:58.998028\n",
      "Episode 21: reward=-21.0, steps=869, speed=111.180 frames/s, elapsed=0:03:06.584690\n",
      "Episode 22: reward=-20.0, steps=1055, speed=111.207 frames/s, elapsed=0:03:15.969177\n",
      "Episode 23: reward=-19.0, steps=936, speed=111.254 frames/s, elapsed=0:03:24.209559\n",
      "Episode 24: reward=-20.0, steps=976, speed=111.267 frames/s, elapsed=0:03:32.923267\n",
      "Episode 25: reward=-20.0, steps=1019, speed=111.225 frames/s, elapsed=0:03:42.267284\n",
      "Episode 26: reward=-21.0, steps=883, speed=111.155 frames/s, elapsed=0:03:50.452869\n",
      "Episode 27: reward=-20.0, steps=877, speed=110.851 frames/s, elapsed=0:03:59.584242\n",
      "Episode 28: reward=-20.0, steps=1039, speed=110.728 frames/s, elapsed=0:04:09.525770\n",
      "Episode 29: reward=-20.0, steps=897, speed=110.784 frames/s, elapsed=0:04:17.426600\n",
      "Episode 30: reward=-21.0, steps=790, speed=110.857 frames/s, elapsed=0:04:24.323270\n",
      "Episode 31: reward=-21.0, steps=762, speed=110.924 frames/s, elapsed=0:04:30.995629\n",
      "Episode 32: reward=-20.0, steps=920, speed=110.990 frames/s, elapsed=0:04:39.057326\n",
      "Episode 33: reward=-21.0, steps=820, speed=111.066 frames/s, elapsed=0:04:46.192691\n",
      "Episode 34: reward=-21.0, steps=790, speed=111.113 frames/s, elapsed=0:04:53.149842\n",
      "Episode 35: reward=-19.0, steps=1069, speed=111.089 frames/s, elapsed=0:05:02.892056\n",
      "Episode 36: reward=-21.0, steps=809, speed=111.164 frames/s, elapsed=0:05:09.918900\n",
      "Episode 37: reward=-21.0, steps=996, speed=111.221 frames/s, elapsed=0:05:18.655004\n",
      "Episode 38: reward=-21.0, steps=878, speed=111.286 frames/s, elapsed=0:05:26.336113\n",
      "Episode 39: reward=-21.0, steps=762, speed=111.347 frames/s, elapsed=0:05:32.999903\n",
      "Episode 40: reward=-20.0, steps=957, speed=111.411 frames/s, elapsed=0:05:41.353003\n",
      "Episode 41: reward=-20.0, steps=989, speed=111.480 frames/s, elapsed=0:05:49.972878\n",
      "Episode 42: reward=-21.0, steps=762, speed=111.538 frames/s, elapsed=0:05:56.635331\n",
      "Episode 43: reward=-21.0, steps=882, speed=111.563 frames/s, elapsed=0:06:04.455244\n",
      "Episode 44: reward=-21.0, steps=906, speed=111.614 frames/s, elapsed=0:06:12.395429\n",
      "Episode 45: reward=-21.0, steps=882, speed=111.664 frames/s, elapsed=0:06:20.123383\n",
      "Episode 46: reward=-20.0, steps=1079, speed=111.660 frames/s, elapsed=0:06:29.785967\n",
      "Episode 47: reward=-21.0, steps=810, speed=111.538 frames/s, elapsed=0:06:37.460406\n",
      "Episode 48: reward=-20.0, steps=944, speed=111.450 frames/s, elapsed=0:06:46.278098\n",
      "Episode 49: reward=-20.0, steps=1061, speed=111.327 frames/s, elapsed=0:06:56.366213\n",
      "Episode 50: reward=-21.0, steps=912, speed=111.262 frames/s, elapsed=0:07:04.802243\n",
      "Episode 51: reward=-21.0, steps=820, speed=111.277 frames/s, elapsed=0:07:12.114447\n",
      "Episode 52: reward=-20.0, steps=947, speed=111.160 frames/s, elapsed=0:07:21.106594\n",
      "Episode 53: reward=-21.0, steps=870, speed=111.198 frames/s, elapsed=0:07:28.804105\n",
      "Episode 54: reward=-21.0, steps=762, speed=111.240 frames/s, elapsed=0:07:35.527603\n",
      "Episode 55: reward=-20.0, steps=836, speed=111.282 frames/s, elapsed=0:07:42.885719\n",
      "Episode 56: reward=-20.0, steps=931, speed=111.316 frames/s, elapsed=0:07:51.146314\n",
      "Episode 57: reward=-20.0, steps=1036, speed=111.321 frames/s, elapsed=0:08:00.420828\n",
      "Episode 58: reward=-20.0, steps=842, speed=111.378 frames/s, elapsed=0:08:07.804530\n",
      "Episode 59: reward=-20.0, steps=1017, speed=111.426 frames/s, elapsed=0:08:16.743307\n",
      "Episode 60: reward=-21.0, steps=822, speed=111.469 frames/s, elapsed=0:08:23.980671\n",
      "Episode 61: reward=-19.0, steps=1098, speed=111.288 frames/s, elapsed=0:08:34.704073\n",
      "Episode 62: reward=-19.0, steps=1062, speed=111.263 frames/s, elapsed=0:08:44.351844\n",
      "Episode 63: reward=-21.0, steps=961, speed=111.251 frames/s, elapsed=0:08:53.027457\n",
      "Episode 64: reward=-21.0, steps=823, speed=111.315 frames/s, elapsed=0:09:00.210117\n",
      "Episode 65: reward=-21.0, steps=852, speed=111.368 frames/s, elapsed=0:09:07.686219\n",
      "Episode 66: reward=-18.0, steps=1165, speed=111.431 frames/s, elapsed=0:09:17.877703\n",
      "Episode 67: reward=-21.0, steps=790, speed=111.490 frames/s, elapsed=0:09:24.776162\n",
      "Episode 68: reward=-19.0, steps=932, speed=111.559 frames/s, elapsed=0:09:32.892670\n",
      "Episode 69: reward=-21.0, steps=822, speed=111.585 frames/s, elapsed=0:09:40.174898\n",
      "Episode 70: reward=-20.0, steps=898, speed=111.630 frames/s, elapsed=0:09:48.054292\n",
      "Episode 71: reward=-21.0, steps=941, speed=111.678 frames/s, elapsed=0:09:56.314896\n",
      "Episode 72: reward=-21.0, steps=959, speed=111.734 frames/s, elapsed=0:10:04.676015\n",
      "Episode 73: reward=-21.0, steps=818, speed=111.734 frames/s, elapsed=0:10:12.007732\n",
      "Episode 74: reward=-21.0, steps=819, speed=111.803 frames/s, elapsed=0:10:19.117719\n",
      "Episode 75: reward=-20.0, steps=925, speed=111.775 frames/s, elapsed=0:10:27.487704\n",
      "Episode 76: reward=-20.0, steps=870, speed=111.830 frames/s, elapsed=0:10:35.083741\n",
      "Episode 77: reward=-19.0, steps=1064, speed=111.799 frames/s, elapsed=0:10:44.739746\n",
      "Episode 78: reward=-19.0, steps=1119, speed=111.823 frames/s, elapsed=0:10:54.640608\n",
      "Episode 79: reward=-21.0, steps=1194, speed=111.872 frames/s, elapsed=0:11:05.090611\n",
      "Episode 80: reward=-20.0, steps=1075, speed=111.921 frames/s, elapsed=0:11:14.483469\n",
      "Episode 81: reward=-19.0, steps=1236, speed=111.971 frames/s, elapsed=0:11:25.287555\n",
      "Episode 82: reward=-17.0, steps=1207, speed=111.880 frames/s, elapsed=0:11:36.541834\n",
      "Episode 83: reward=-18.0, steps=1400, speed=111.771 frames/s, elapsed=0:11:49.678007\n",
      "Episode 84: reward=-18.0, steps=1384, speed=111.716 frames/s, elapsed=0:12:02.393405\n",
      "Episode 85: reward=-19.0, steps=1443, speed=111.674 frames/s, elapsed=0:12:15.558266\n",
      "Episode 86: reward=-20.0, steps=1205, speed=111.667 frames/s, elapsed=0:12:26.363267\n",
      "Episode 87: reward=-18.0, steps=1359, speed=111.571 frames/s, elapsed=0:12:39.081193\n",
      "Episode 88: reward=-17.0, steps=1582, speed=111.534 frames/s, elapsed=0:12:53.515368\n",
      "Episode 89: reward=-16.0, steps=1770, speed=111.483 frames/s, elapsed=0:13:09.758354\n",
      "Episode 90: reward=-18.0, steps=1324, speed=111.505 frames/s, elapsed=0:13:21.510276\n",
      "Episode 91: reward=-17.0, steps=1591, speed=111.446 frames/s, elapsed=0:13:36.151995\n",
      "Episode 92: reward=-18.0, steps=1466, speed=111.400 frames/s, elapsed=0:13:49.596817\n",
      "Episode 93: reward=-16.0, steps=1695, speed=111.418 frames/s, elapsed=0:14:04.691361\n",
      "Episode 94: reward=-15.0, steps=1832, speed=111.337 frames/s, elapsed=0:14:21.761939\n",
      "Episode 95: reward=-18.0, steps=1500, speed=111.186 frames/s, elapsed=0:14:36.214983\n",
      "Episode 96: reward=-21.0, steps=1397, speed=111.094 frames/s, elapsed=0:14:49.299064\n",
      "Episode 97: reward=-16.0, steps=1702, speed=111.068 frames/s, elapsed=0:15:04.822855\n",
      "Episode 98: reward=-15.0, steps=1734, speed=110.748 frames/s, elapsed=0:15:23.060065\n",
      "Episode 99: reward=-18.0, steps=1643, speed=110.760 frames/s, elapsed=0:15:37.797830\n",
      "Episode 100: reward=-16.0, steps=1735, speed=110.830 frames/s, elapsed=0:15:52.999614\n",
      "Episode 101: reward=-17.0, steps=1570, speed=110.762 frames/s, elapsed=0:16:07.602932\n",
      "Episode 102: reward=-19.0, steps=1410, speed=110.611 frames/s, elapsed=0:16:21.268318\n",
      "Episode 103: reward=-18.0, steps=1876, speed=110.609 frames/s, elapsed=0:16:38.230411\n",
      "Episode 104: reward=-16.0, steps=2080, speed=110.644 frames/s, elapsed=0:16:56.763288\n",
      "Episode 105: reward=-18.0, steps=2076, speed=110.708 frames/s, elapsed=0:17:14.996509\n",
      "Episode 106: reward=-18.0, steps=1818, speed=110.724 frames/s, elapsed=0:17:31.301856\n",
      "Episode 107: reward=-19.0, steps=1900, speed=110.711 frames/s, elapsed=0:17:48.556608\n",
      "Episode 108: reward=-18.0, steps=2199, speed=110.640 frames/s, elapsed=0:18:09.078364\n",
      "Episode 109: reward=-17.0, steps=1894, speed=110.541 frames/s, elapsed=0:18:26.988219\n",
      "Episode 110: reward=-19.0, steps=1670, speed=110.507 frames/s, elapsed=0:18:42.341858\n",
      "Episode 111: reward=-18.0, steps=1809, speed=110.321 frames/s, elapsed=0:19:00.208523\n",
      "Episode 112: reward=-18.0, steps=2005, speed=110.154 frames/s, elapsed=0:19:19.863686\n",
      "Episode 113: reward=-19.0, steps=1677, speed=110.171 frames/s, elapsed=0:19:34.973568\n",
      "Episode 114: reward=-19.0, steps=1724, speed=110.174 frames/s, elapsed=0:19:50.607096\n",
      "Episode 115: reward=-19.0, steps=1277, speed=110.053 frames/s, elapsed=0:20:02.879394\n",
      "Episode 116: reward=-21.0, steps=1680, speed=109.931 frames/s, elapsed=0:20:19.046195\n",
      "Episode 117: reward=-21.0, steps=1602, speed=109.867 frames/s, elapsed=0:20:34.051636\n",
      "Episode 118: reward=-21.0, steps=1724, speed=109.811 frames/s, elapsed=0:20:50.138630\n",
      "Episode 119: reward=-18.0, steps=1872, speed=109.841 frames/s, elapsed=0:21:06.955766\n",
      "Episode 120: reward=-18.0, steps=2134, speed=109.634 frames/s, elapsed=0:21:28.429418\n",
      "Episode 121: reward=-16.0, steps=2607, speed=109.439 frames/s, elapsed=0:21:54.526891\n",
      "Episode 122: reward=-18.0, steps=2651, speed=109.409 frames/s, elapsed=0:22:19.060072\n",
      "Episode 123: reward=-18.0, steps=2418, speed=109.514 frames/s, elapsed=0:22:40.149468\n",
      "Episode 124: reward=-18.0, steps=2357, speed=109.551 frames/s, elapsed=0:23:01.325586\n",
      "Episode 125: reward=-16.0, steps=2363, speed=109.543 frames/s, elapsed=0:23:22.987892\n",
      "Episode 126: reward=-17.0, steps=2814, speed=109.460 frames/s, elapsed=0:23:49.680359\n",
      "Episode 127: reward=-14.0, steps=3023, speed=109.427 frames/s, elapsed=0:24:17.711803\n",
      "Episode 128: reward=-16.0, steps=2664, speed=109.486 frames/s, elapsed=0:24:41.414479\n",
      "Episode 129: reward=-12.0, steps=3180, speed=109.533 frames/s, elapsed=0:25:09.845391\n",
      "Episode 130: reward=-16.0, steps=2736, speed=109.484 frames/s, elapsed=0:25:35.398324\n",
      "Episode 131: reward=-14.0, steps=2971, speed=109.526 frames/s, elapsed=0:26:02.044555\n",
      "Episode 132: reward=-13.0, steps=2684, speed=109.547 frames/s, elapsed=0:26:26.299283\n",
      "Episode 133: reward=-12.0, steps=2451, speed=109.550 frames/s, elapsed=0:26:48.636720\n",
      "Episode 134: reward=-13.0, steps=2237, speed=109.594 frames/s, elapsed=0:27:08.666750\n",
      "Episode 135: reward=-14.0, steps=2500, speed=109.644 frames/s, elapsed=0:27:30.958015\n",
      "Episode 136: reward=-9.0, steps=3832, speed=109.577 frames/s, elapsed=0:28:07.028414\n",
      "Episode 137: reward=-3.0, steps=4158, speed=109.364 frames/s, elapsed=0:28:49.059984\n",
      "Episode 138: reward=-12.0, steps=2800, speed=109.439 frames/s, elapsed=0:29:13.805752\n",
      "Episode 139: reward=-14.0, steps=3108, speed=109.536 frames/s, elapsed=0:29:41.005042\n",
      "Episode 140: reward=-10.0, steps=4156, speed=109.616 frames/s, elapsed=0:30:17.593907\n",
      "Episode 141: reward=-3.0, steps=4067, speed=109.707 frames/s, elapsed=0:30:53.228785\n",
      "Episode 142: reward=-4.0, steps=5293, speed=109.830 frames/s, elapsed=0:31:38.910090\n",
      "Episode 143: reward=7.0, steps=4113, speed=109.939 frames/s, elapsed=0:32:14.581822\n",
      "Episode 144: reward=-13.0, steps=2663, speed=110.060 frames/s, elapsed=0:32:37.546604\n",
      "Episode 145: reward=-13.0, steps=2440, speed=110.040 frames/s, elapsed=0:32:59.914241\n",
      "Episode 146: reward=-1.0, steps=4692, speed=109.970 frames/s, elapsed=0:33:43.950081\n",
      "Episode 147: reward=1.0, steps=3830, speed=109.710 frames/s, elapsed=0:34:23.456291\n",
      "Episode 148: reward=-1.0, steps=4278, speed=109.815 frames/s, elapsed=0:35:00.672134\n",
      "Episode 149: reward=9.0, steps=3638, speed=109.941 frames/s, elapsed=0:35:32.012014\n",
      "Episode 150: reward=-10.0, steps=3568, speed=109.983 frames/s, elapsed=0:36:03.856899\n",
      "Episode 151: reward=-4.0, steps=3608, speed=109.854 frames/s, elapsed=0:36:38.700357\n",
      "Episode 152: reward=14.0, steps=2850, speed=109.910 frames/s, elapsed=0:37:04.000009\n",
      "Episode 153: reward=2.0, steps=4819, speed=109.946 frames/s, elapsed=0:37:47.141074\n",
      "Episode 154: reward=-6.0, steps=3499, speed=110.070 frames/s, elapsed=0:38:17.252980\n",
      "Episode 155: reward=20.0, steps=2034, speed=110.185 frames/s, elapsed=0:38:34.812113\n",
      "Episode 156: reward=18.0, steps=2265, speed=110.306 frames/s, elapsed=0:38:54.306635\n",
      "Episode 157: reward=-10.0, steps=2596, speed=110.418 frames/s, elapsed=0:39:16.715905\n",
      "Episode 158: reward=-3.0, steps=3749, speed=110.435 frames/s, elapsed=0:39:50.395639\n",
      "Episode 159: reward=-11.0, steps=2547, speed=110.494 frames/s, elapsed=0:40:12.856426\n",
      "Episode 160: reward=-5.0, steps=3398, speed=110.560 frames/s, elapsed=0:40:42.732152\n",
      "Episode 161: reward=-6.0, steps=3355, speed=110.661 frames/s, elapsed=0:41:11.735666\n",
      "Episode 162: reward=-6.0, steps=3611, speed=110.693 frames/s, elapsed=0:41:43.918117\n",
      "Episode 163: reward=-7.0, steps=3994, speed=110.777 frames/s, elapsed=0:42:18.669070\n",
      "Episode 164: reward=-9.0, steps=3081, speed=110.839 frames/s, elapsed=0:42:45.724755\n",
      "Episode 165: reward=-5.0, steps=3225, speed=110.931 frames/s, elapsed=0:43:13.661729\n",
      "Episode 166: reward=-6.0, steps=3154, speed=110.995 frames/s, elapsed=0:43:41.316533\n",
      "Episode 167: reward=-1.0, steps=4324, speed=111.079 frames/s, elapsed=0:44:18.846549\n",
      "Episode 168: reward=5.0, steps=3569, speed=111.153 frames/s, elapsed=0:44:49.950100\n",
      "Episode 169: reward=2.0, steps=3976, speed=111.162 frames/s, elapsed=0:45:25.565735\n",
      "Episode 170: reward=-3.0, steps=4122, speed=111.103 frames/s, elapsed=0:46:03.658946\n",
      "Episode 171: reward=-7.0, steps=2807, speed=111.086 frames/s, elapsed=0:46:29.124901\n",
      "Episode 172: reward=-7.0, steps=3548, speed=111.160 frames/s, elapsed=0:47:00.011766\n",
      "Episode 173: reward=-4.0, steps=3505, speed=111.236 frames/s, elapsed=0:47:30.520258\n",
      "Episode 174: reward=5.0, steps=3552, speed=111.317 frames/s, elapsed=0:48:01.332571\n",
      "Episode 175: reward=-2.0, steps=4341, speed=111.389 frames/s, elapsed=0:48:39.111954\n",
      "Episode 176: reward=-3.0, steps=4145, speed=111.443 frames/s, elapsed=0:49:15.421959\n",
      "Episode 177: reward=5.0, steps=3955, speed=111.494 frames/s, elapsed=0:49:50.128707\n",
      "Episode 178: reward=9.0, steps=3458, speed=111.406 frames/s, elapsed=0:50:22.406851\n",
      "Episode 179: reward=14.0, steps=2737, speed=111.397 frames/s, elapsed=0:50:47.092184\n",
      "Episode 180: reward=13.0, steps=2962, speed=111.381 frames/s, elapsed=0:51:13.863595\n",
      "Episode 181: reward=10.0, steps=3659, speed=111.339 frames/s, elapsed=0:51:47.360829\n",
      "Episode 182: reward=-1.0, steps=4529, speed=111.326 frames/s, elapsed=0:52:28.258971\n",
      "Episode 183: reward=16.0, steps=2643, speed=111.305 frames/s, elapsed=0:52:52.220087\n",
      "Episode 184: reward=17.0, steps=2653, speed=111.301 frames/s, elapsed=0:53:16.122730\n",
      "Episode 185: reward=-3.0, steps=4209, speed=111.270 frames/s, elapsed=0:53:54.462765\n",
      "Episode 186: reward=7.0, steps=4019, speed=111.255 frames/s, elapsed=0:54:30.816756\n",
      "Episode 187: reward=16.0, steps=2525, speed=111.326 frames/s, elapsed=0:54:52.814849\n",
      "Episode 188: reward=16.0, steps=2498, speed=111.307 frames/s, elapsed=0:55:15.458814\n",
      "Episode 189: reward=6.0, steps=4009, speed=111.351 frames/s, elapsed=0:55:50.765479\n",
      "Episode 190: reward=17.0, steps=2648, speed=111.397 frames/s, elapsed=0:56:14.070820\n",
      "Episode 191: reward=16.0, steps=2486, speed=111.434 frames/s, elapsed=0:56:36.012019\n",
      "Episode 192: reward=16.0, steps=2876, speed=111.466 frames/s, elapsed=0:57:01.458139\n",
      "Episode 193: reward=2.0, steps=4047, speed=111.535 frames/s, elapsed=0:57:36.682651\n",
      "Episode 194: reward=2.0, steps=4271, speed=111.597 frames/s, elapsed=0:58:13.938989\n",
      "Episode 195: reward=8.0, steps=3657, speed=111.678 frames/s, elapsed=0:58:45.572617\n",
      "Episode 196: reward=13.0, steps=2583, speed=111.732 frames/s, elapsed=0:59:08.153778\n",
      "Episode 197: reward=16.0, steps=2569, speed=111.795 frames/s, elapsed=0:59:30.500444\n",
      "Episode 198: reward=2.0, steps=4092, speed=111.867 frames/s, elapsed=1:00:05.974216\n",
      "Episode 199: reward=13.0, steps=2744, speed=111.905 frames/s, elapsed=1:00:30.096674\n",
      "Episode 200: reward=6.0, steps=3279, speed=111.969 frames/s, elapsed=1:00:58.582290\n",
      "Episode 201: reward=18.0, steps=2553, speed=111.985 frames/s, elapsed=1:01:21.221510\n",
      "Episode 202: reward=17.0, steps=2431, speed=112.035 frames/s, elapsed=1:01:42.448476\n",
      "Episode 203: reward=14.0, steps=2762, speed=112.096 frames/s, elapsed=1:02:06.457387\n",
      "Episode 204: reward=21.0, steps=2155, speed=112.148 frames/s, elapsed=1:02:25.239719\n",
      "Episode 205: reward=1.0, steps=4887, speed=112.043 frames/s, elapsed=1:03:10.960595\n",
      "Episode 206: reward=14.0, steps=3214, speed=111.889 frames/s, elapsed=1:03:41.744364\n",
      "Episode 207: reward=7.0, steps=3649, speed=111.849 frames/s, elapsed=1:04:14.973504\n",
      "Episode 208: reward=17.0, steps=2569, speed=111.838 frames/s, elapsed=1:04:38.050069\n",
      "Episode 209: reward=-3.0, steps=4616, speed=111.820 frames/s, elapsed=1:05:19.650841\n",
      "Episode 210: reward=14.0, steps=2818, speed=111.827 frames/s, elapsed=1:05:44.768838\n",
      "Episode 211: reward=15.0, steps=2871, speed=111.782 frames/s, elapsed=1:06:10.970584\n",
      "Episode 212: reward=19.0, steps=2337, speed=111.690 frames/s, elapsed=1:06:32.776359\n",
      "Episode 213: reward=21.0, steps=1993, speed=111.662 frames/s, elapsed=1:06:50.836711\n",
      "Episode 214: reward=16.0, steps=2935, speed=111.542 frames/s, elapsed=1:07:18.632146\n",
      "Episode 215: reward=15.0, steps=2639, speed=111.519 frames/s, elapsed=1:07:42.524626\n",
      "Episode 216: reward=10.0, steps=3640, speed=111.359 frames/s, elapsed=1:08:17.693788\n",
      "Episode 217: reward=21.0, steps=2081, speed=111.248 frames/s, elapsed=1:08:37.347387\n",
      "Episode 218: reward=12.0, steps=3596, speed=111.182 frames/s, elapsed=1:09:10.658349\n",
      "Episode 219: reward=21.0, steps=1975, speed=111.137 frames/s, elapsed=1:09:28.779560\n",
      "Episode 220: reward=21.0, steps=1975, speed=111.127 frames/s, elapsed=1:09:46.652953\n",
      "Episode 221: reward=20.0, steps=2041, speed=111.037 frames/s, elapsed=1:10:05.781843\n",
      "Episode 222: reward=-1.0, steps=4078, speed=110.967 frames/s, elapsed=1:10:43.707698\n",
      "Episode 223: reward=13.0, steps=2940, speed=110.799 frames/s, elapsed=1:11:12.371700\n",
      "Episode 224: reward=15.0, steps=3112, speed=110.716 frames/s, elapsed=1:11:41.566300\n",
      "Episode 225: reward=18.0, steps=2229, speed=110.631 frames/s, elapsed=1:12:02.496753\n",
      "Episode 226: reward=18.0, steps=2559, speed=110.577 frames/s, elapsed=1:12:26.214374\n",
      "Episode 227: reward=12.0, steps=2828, speed=110.527 frames/s, elapsed=1:12:52.356961\n",
      "Episode 228: reward=16.0, steps=2436, speed=110.487 frames/s, elapsed=1:13:14.801564\n",
      "Episode 229: reward=20.0, steps=2109, speed=110.483 frames/s, elapsed=1:13:33.928064\n",
      "Episode 230: reward=13.0, steps=3006, speed=110.464 frames/s, elapsed=1:14:01.367206\n",
      "Episode 231: reward=11.0, steps=3111, speed=110.558 frames/s, elapsed=1:14:28.381615\n",
      "Episode 232: reward=14.0, steps=2414, speed=110.620 frames/s, elapsed=1:14:49.634473\n",
      "Episode 233: reward=8.0, steps=3526, speed=110.709 frames/s, elapsed=1:15:20.262100\n",
      "Episode 234: reward=13.0, steps=2656, speed=110.777 frames/s, elapsed=1:15:43.552021\n",
      "Episode 235: reward=14.0, steps=2641, speed=110.706 frames/s, elapsed=1:16:08.182785\n",
      "Episode 236: reward=12.0, steps=2901, speed=110.788 frames/s, elapsed=1:16:33.447230\n",
      "Episode 237: reward=8.0, steps=3473, speed=110.852 frames/s, elapsed=1:17:03.919118\n",
      "Episode 238: reward=6.0, steps=4109, speed=110.934 frames/s, elapsed=1:17:39.648348\n",
      "Episode 239: reward=11.0, steps=2966, speed=110.871 frames/s, elapsed=1:18:07.178298\n",
      "Episode 240: reward=9.0, steps=3268, speed=110.961 frames/s, elapsed=1:18:35.495841\n",
      "Episode 241: reward=10.0, steps=2749, speed=111.025 frames/s, elapsed=1:18:59.594843\n",
      "Episode 242: reward=9.0, steps=3173, speed=111.095 frames/s, elapsed=1:19:27.276324\n",
      "Episode 243: reward=6.0, steps=4210, speed=111.167 frames/s, elapsed=1:20:04.001056\n",
      "Episode 244: reward=15.0, steps=2678, speed=111.244 frames/s, elapsed=1:20:27.275508\n",
      "Episode 245: reward=13.0, steps=2772, speed=111.320 frames/s, elapsed=1:20:51.366451\n",
      "Episode 246: reward=11.0, steps=2870, speed=111.374 frames/s, elapsed=1:21:16.545937\n",
      "Episode 247: reward=19.0, steps=2128, speed=111.426 frames/s, elapsed=1:21:35.204919\n",
      "Episode 248: reward=12.0, steps=3440, speed=111.507 frames/s, elapsed=1:22:05.012271\n",
      "Episode 249: reward=21.0, steps=1975, speed=111.549 frames/s, elapsed=1:22:22.387333\n",
      "Episode 250: reward=20.0, steps=1975, speed=111.600 frames/s, elapsed=1:22:39.712814\n",
      "Episode 251: reward=19.0, steps=2117, speed=111.660 frames/s, elapsed=1:22:58.164558\n",
      "Episode 252: reward=21.0, steps=2035, speed=111.732 frames/s, elapsed=1:23:15.844150\n",
      "Episode 253: reward=18.0, steps=2459, speed=111.759 frames/s, elapsed=1:23:37.566342\n",
      "Episode 254: reward=21.0, steps=1975, speed=111.820 frames/s, elapsed=1:23:54.786635\n",
      "Episode 255: reward=14.0, steps=2736, speed=111.887 frames/s, elapsed=1:24:18.547932\n",
      "Episode 256: reward=15.0, steps=2454, speed=111.917 frames/s, elapsed=1:24:40.188553\n",
      "Episode 257: reward=14.0, steps=3156, speed=111.978 frames/s, elapsed=1:25:07.638479\n",
      "Episode 258: reward=16.0, steps=2584, speed=112.014 frames/s, elapsed=1:25:30.335349\n",
      "Episode 259: reward=17.0, steps=2480, speed=112.052 frames/s, elapsed=1:25:52.120682\n",
      "Episode 260: reward=9.0, steps=3069, speed=112.111 frames/s, elapsed=1:26:18.810415\n",
      "Episode 261: reward=17.0, steps=2345, speed=112.165 frames/s, elapsed=1:26:39.217201\n",
      "Episode 262: reward=16.0, steps=2665, speed=112.209 frames/s, elapsed=1:27:02.535625\n",
      "Episode 263: reward=16.0, steps=2343, speed=112.262 frames/s, elapsed=1:27:22.928658\n",
      "Episode 264: reward=3.0, steps=4009, speed=112.297 frames/s, elapsed=1:27:58.089011\n",
      "Episode 265: reward=6.0, steps=3555, speed=112.350 frames/s, elapsed=1:28:29.011970\n",
      "Episode 266: reward=19.0, steps=2292, speed=112.400 frames/s, elapsed=1:28:48.969596\n",
      "Episode 267: reward=12.0, steps=2894, speed=112.442 frames/s, elapsed=1:29:14.258046\n",
      "Episode 268: reward=14.0, steps=2886, speed=112.516 frames/s, elapsed=1:29:39.104754\n",
      "Episode 269: reward=17.0, steps=2343, speed=112.561 frames/s, elapsed=1:29:59.517134\n",
      "Episode 270: reward=16.0, steps=2633, speed=112.598 frames/s, elapsed=1:30:22.521582\n",
      "Episode 271: reward=17.0, steps=2135, speed=112.658 frames/s, elapsed=1:30:40.994039\n",
      "Episode 272: reward=17.0, steps=2412, speed=112.696 frames/s, elapsed=1:31:02.048387\n",
      "Episode 273: reward=19.0, steps=2054, speed=112.690 frames/s, elapsed=1:31:20.334917\n",
      "Episode 274: reward=18.0, steps=2209, speed=112.741 frames/s, elapsed=1:31:39.494952\n",
      "Episode 275: reward=10.0, steps=3368, speed=112.775 frames/s, elapsed=1:32:08.928050\n",
      "Episode 276: reward=5.0, steps=3588, speed=112.806 frames/s, elapsed=1:32:40.324163\n",
      "Episode 277: reward=15.0, steps=2629, speed=112.857 frames/s, elapsed=1:33:03.103189\n",
      "Episode 278: reward=14.0, steps=2764, speed=112.903 frames/s, elapsed=1:33:27.093733\n",
      "Episode 279: reward=11.0, steps=2863, speed=112.947 frames/s, elapsed=1:33:51.985137\n",
      "Episode 280: reward=14.0, steps=2605, speed=113.001 frames/s, elapsed=1:34:14.499251\n",
      "Episode 281: reward=7.0, steps=3256, speed=113.029 frames/s, elapsed=1:34:42.956079\n",
      "Episode 282: reward=9.0, steps=3326, speed=113.067 frames/s, elapsed=1:35:11.905942\n",
      "Episode 283: reward=11.0, steps=3523, speed=113.090 frames/s, elapsed=1:35:42.740813\n",
      "Episode 284: reward=14.0, steps=2569, speed=113.129 frames/s, elapsed=1:36:05.086365\n",
      "Episode 285: reward=20.0, steps=2040, speed=113.184 frames/s, elapsed=1:36:22.695966\n",
      "Episode 286: reward=14.0, steps=2646, speed=113.213 frames/s, elapsed=1:36:45.777406\n",
      "Episode 287: reward=17.0, steps=2726, speed=113.245 frames/s, elapsed=1:37:09.500970\n",
      "Episode 288: reward=18.0, steps=2246, speed=113.286 frames/s, elapsed=1:37:28.992573\n",
      "Episode 289: reward=16.0, steps=2673, speed=113.315 frames/s, elapsed=1:37:52.287190\n",
      "Episode 290: reward=15.0, steps=2865, speed=113.353 frames/s, elapsed=1:38:17.151502\n",
      "Episode 291: reward=10.0, steps=3064, speed=113.400 frames/s, elapsed=1:38:43.627797\n",
      "Episode 292: reward=15.0, steps=2766, speed=113.410 frames/s, elapsed=1:39:07.911577\n",
      "Episode 293: reward=10.0, steps=3750, speed=113.464 frames/s, elapsed=1:39:40.208196\n",
      "Episode 294: reward=12.0, steps=3158, speed=113.483 frames/s, elapsed=1:40:07.811226\n",
      "Episode 295: reward=14.0, steps=2912, speed=113.508 frames/s, elapsed=1:40:33.201282\n",
      "Episode 296: reward=18.0, steps=2224, speed=113.542 frames/s, elapsed=1:40:52.498839\n",
      "Episode 297: reward=16.0, steps=2574, speed=113.552 frames/s, elapsed=1:41:15.073301\n",
      "Episode 298: reward=10.0, steps=3115, speed=113.578 frames/s, elapsed=1:41:42.183339\n",
      "Episode 299: reward=13.0, steps=2838, speed=113.611 frames/s, elapsed=1:42:06.814885\n",
      "Episode 300: reward=10.0, steps=3258, speed=113.623 frames/s, elapsed=1:42:35.333250\n",
      "Episode 301: reward=15.0, steps=2851, speed=113.653 frames/s, elapsed=1:43:00.118630\n",
      "Episode 302: reward=15.0, steps=2717, speed=113.679 frames/s, elapsed=1:43:23.740860\n",
      "Episode 303: reward=17.0, steps=2838, speed=113.703 frames/s, elapsed=1:43:48.439100\n",
      "Episode 304: reward=13.0, steps=2990, speed=113.728 frames/s, elapsed=1:44:14.467483\n",
      "Episode 305: reward=19.0, steps=2254, speed=113.740 frames/s, elapsed=1:44:34.171455\n",
      "Episode 306: reward=16.0, steps=2475, speed=113.764 frames/s, elapsed=1:44:55.701632\n",
      "Episode 307: reward=12.0, steps=2762, speed=113.779 frames/s, elapsed=1:45:19.827153\n",
      "Episode 308: reward=16.0, steps=2452, speed=113.778 frames/s, elapsed=1:45:41.382859\n",
      "Episode 309: reward=15.0, steps=2602, speed=113.805 frames/s, elapsed=1:46:03.999167\n",
      "Episode 310: reward=14.0, steps=2856, speed=113.839 frames/s, elapsed=1:46:28.725256\n",
      "Episode 311: reward=13.0, steps=3122, speed=113.843 frames/s, elapsed=1:46:56.085056\n",
      "Episode 312: reward=17.0, steps=2344, speed=113.847 frames/s, elapsed=1:47:16.658726\n",
      "Episode 313: reward=12.0, steps=2840, speed=113.862 frames/s, elapsed=1:47:41.423830\n",
      "Episode 314: reward=15.0, steps=2748, speed=113.874 frames/s, elapsed=1:48:05.429177\n",
      "Episode 315: reward=8.0, steps=3774, speed=113.897 frames/s, elapsed=1:48:38.237081\n",
      "Episode 316: reward=20.0, steps=2216, speed=113.898 frames/s, elapsed=1:48:57.697125\n",
      "Episode 317: reward=11.0, steps=3549, speed=113.923 frames/s, elapsed=1:49:28.521666\n",
      "Episode 318: reward=11.0, steps=3015, speed=113.935 frames/s, elapsed=1:49:54.841547\n",
      "Episode 319: reward=18.0, steps=2232, speed=113.938 frames/s, elapsed=1:50:14.406896\n",
      "Episode 320: reward=18.0, steps=2421, speed=113.967 frames/s, elapsed=1:50:35.392069\n",
      "Episode 321: reward=15.0, steps=2633, speed=113.999 frames/s, elapsed=1:50:58.182175\n",
      "Episode 322: reward=14.0, steps=2602, speed=113.986 frames/s, elapsed=1:51:21.127980\n",
      "Episode 323: reward=18.0, steps=2469, speed=114.024 frames/s, elapsed=1:51:42.431633\n",
      "Episode 324: reward=15.0, steps=2535, speed=114.037 frames/s, elapsed=1:52:04.540905\n",
      "Episode 325: reward=12.0, steps=2811, speed=114.046 frames/s, elapsed=1:52:29.097064\n",
      "Episode 326: reward=16.0, steps=2422, speed=114.075 frames/s, elapsed=1:52:50.050632\n",
      "Episode 327: reward=13.0, steps=2899, speed=114.089 frames/s, elapsed=1:53:15.332867\n",
      "Episode 328: reward=16.0, steps=2376, speed=114.103 frames/s, elapsed=1:53:36.029001\n",
      "Episode 329: reward=15.0, steps=2609, speed=114.072 frames/s, elapsed=1:53:59.195639\n",
      "Episode 330: reward=14.0, steps=3345, speed=114.072 frames/s, elapsed=1:54:28.509220\n",
      "Episode 331: reward=20.0, steps=2248, speed=114.071 frames/s, elapsed=1:54:48.247095\n",
      "Episode 332: reward=11.0, steps=2928, speed=114.092 frames/s, elapsed=1:55:13.675331\n",
      "Episode 333: reward=15.0, steps=2698, speed=114.092 frames/s, elapsed=1:55:37.318827\n",
      "Episode 334: reward=19.0, steps=2146, speed=114.123 frames/s, elapsed=1:55:55.869983\n",
      "Episode 335: reward=11.0, steps=3608, speed=114.142 frames/s, elapsed=1:56:27.232803\n",
      "Episode 336: reward=6.0, steps=4077, speed=114.153 frames/s, elapsed=1:57:02.774441\n",
      "Episode 337: reward=11.0, steps=3306, speed=114.167 frames/s, elapsed=1:57:31.560423\n",
      "Episode 338: reward=11.0, steps=2855, speed=114.170 frames/s, elapsed=1:57:56.544414\n",
      "Episode 339: reward=14.0, steps=2702, speed=114.182 frames/s, elapsed=1:58:20.071019\n",
      "Episode 340: reward=20.0, steps=2141, speed=114.183 frames/s, elapsed=1:58:38.817351\n",
      "Episode 341: reward=13.0, steps=3439, speed=114.188 frames/s, elapsed=1:59:08.861780\n",
      "Episode 342: reward=15.0, steps=2854, speed=114.219 frames/s, elapsed=1:59:33.544353\n",
      "Episode 343: reward=14.0, steps=2890, speed=114.232 frames/s, elapsed=1:59:58.692873\n",
      "Episode 344: reward=13.0, steps=2646, speed=114.232 frames/s, elapsed=2:00:21.852673\n",
      "Episode 345: reward=18.0, steps=2186, speed=114.255 frames/s, elapsed=2:00:40.811496\n",
      "Episode 346: reward=11.0, steps=2975, speed=114.261 frames/s, elapsed=2:01:06.764308\n",
      "Episode 347: reward=8.0, steps=3272, speed=114.271 frames/s, elapsed=2:01:35.277519\n",
      "Episode 348: reward=19.0, steps=2092, speed=114.260 frames/s, elapsed=2:01:53.663143\n",
      "Episode 349: reward=17.0, steps=2848, speed=114.239 frames/s, elapsed=2:02:18.845560\n",
      "Episode 350: reward=13.0, steps=2774, speed=114.262 frames/s, elapsed=2:02:42.867177\n",
      "Episode 351: reward=17.0, steps=2586, speed=114.283 frames/s, elapsed=2:03:05.290871\n",
      "Episode 352: reward=11.0, steps=3188, speed=114.283 frames/s, elapsed=2:03:33.190636\n",
      "Episode 353: reward=15.0, steps=2728, speed=114.305 frames/s, elapsed=2:03:56.831658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9888\\1886916264.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_handler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mptan_ignite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPeriodEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERS_100_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                 \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                 \u001b[1;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9888\\3467553847.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[1;34m(engine, batch)\u001b[0m\n\u001b[0;32m      3\u001b[0m     loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n\u001b[0;32m      4\u001b[0m                                   gamma=params.gamma, device=device)\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mloss_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mepsilon_tracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_envs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "def episode_completed(trainer: Engine):\n",
    "    print(\"Episode %d: reward=%s, steps=%s, speed=%.3f frames/s, elapsed=%s\" % (\n",
    "        trainer.state.episode, trainer.state.episode_reward,\n",
    "        trainer.state.episode_steps, trainer.state.metrics.get('avg_fps', 0),\n",
    "        timedelta(seconds=trainer.state.metrics.get('time_passed', 0))))\n",
    "    \n",
    "@engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "def game_solved(trainer: Engine):\n",
    "    print(\"Game solved in %s, after %d episodes and %d iterations!\" % (\n",
    "        timedelta(seconds=trainer.state.metrics['time_passed']),\n",
    "        trainer.state.episode, trainer.state.iteration))\n",
    "    trainer.should_terminate = True\n",
    "    \n",
    "logdir = f\"runs/{datetime.now().minute}-{params.run_name}-{NAME}\"\n",
    "tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "RunningAverage(output_transform=lambda v: v['loss']).attach(engine, \"avg_loss\")\n",
    "\n",
    "episode_handler = tb_logger.OutputHandler(tag='episodes', metric_names=['reward', 'steps', 'avg_reward'])\n",
    "tb.attach(engine, log_handler=episode_handler, event_name=ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "\n",
    "# writing to tensorboard every 100 iterations\n",
    "ptan_ignite.PeriodicEvents().attach(engine)\n",
    "handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['avg_loss', 'avg_fps'],\n",
    "                                  output_transform=lambda a: a)\n",
    "tb.attach(engine, log_handler=handler, event_name=ptan_ignite.PeriodEvents.ITERS_100_COMPLETED)\n",
    "\n",
    "engine.run(batch_generator(buffer, params.replay_initial, params.batch_size, n_envs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307dd0f0-5eb9-48bf-b19c-a786fe76dec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
