{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af7b817-07e1-4c60-a839-7b107197547b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "from lib import dqn_model, common\n",
    "\n",
    "NAME = \"01_baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145b4197-9a52-4235-9787-9868bdb1b546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer,\n",
    "                    initial: int, batch_size: int):\n",
    "    buffer.populate(initial)\n",
    "    while True:\n",
    "        buffer.populate(1)\n",
    "        yield buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1a4a8d4-774c-4744-915d-1acce7a4d4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123, 151010689]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting rid of missing metrics warning\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "random.seed(common.SEED)\n",
    "torch.manual_seed(common.SEED)\n",
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(params.env_name)\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63c18f15-70d1-490d-ac6f-e2af6aa39cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=params.gamma)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(\n",
    "    exp_source, buffer_size=params.replay_size)\n",
    "optimizer = optim.Adam(net.parameters(), lr = params.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6ef8609-212c-48e7-bab5-bb86338b6543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_batch(engine, batch):\n",
    "    optimizer.zero_grad()\n",
    "    loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n",
    "                                  gamma=params.gamma, device=device)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    epsilon_tracker.frame(engine.state.iteration)\n",
    "    if engine.state.iteration % params.target_net_sync == 0:\n",
    "        tgt_net.sync()\n",
    "    return {\n",
    "        \"loss\": loss_v.item(),\n",
    "        \"epsilon\": selector.epsilon,\n",
    "    }\n",
    "\n",
    "engine=Engine(process_batch)\n",
    "ptan_ignite.EndOfEpisodeHandler(exp_source, bound_avg_reward=15.0).attach(engine)\n",
    "ptan_ignite.EpisodeFPSHandler().attach(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55c39e86-56a2-4ebe-bf87-c385d0341bdc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21.0, steps=778, speed=0.000 frames/s, elapsed=0:01:06.750490\n",
      "Episode 2: reward=-21.0, steps=762, speed=0.000 frames/s, elapsed=0:01:06.750490\n",
      "Episode 3: reward=-21.0, steps=759, speed=0.000 frames/s, elapsed=0:01:06.751493\n",
      "Episode 4: reward=-20.0, steps=839, speed=0.000 frames/s, elapsed=0:01:06.751493\n",
      "Episode 5: reward=-21.0, steps=755, speed=0.000 frames/s, elapsed=0:01:06.751493\n",
      "Episode 6: reward=-21.0, steps=760, speed=0.000 frames/s, elapsed=0:01:06.751493\n",
      "Episode 7: reward=-21.0, steps=761, speed=0.000 frames/s, elapsed=0:01:06.751493\n",
      "Episode 8: reward=-21.0, steps=759, speed=0.000 frames/s, elapsed=0:01:06.752491\n",
      "Episode 9: reward=-21.0, steps=759, speed=0.000 frames/s, elapsed=0:01:06.752491\n",
      "Episode 10: reward=-21.0, steps=755, speed=0.000 frames/s, elapsed=0:01:06.752491\n",
      "Episode 11: reward=-21.0, steps=761, speed=0.000 frames/s, elapsed=0:01:06.752491\n",
      "Episode 12: reward=-21.0, steps=757, speed=0.000 frames/s, elapsed=0:01:06.753490\n",
      "Episode 13: reward=-20.0, steps=834, speed=0.000 frames/s, elapsed=0:01:06.753490\n",
      "Episode 14: reward=-21.0, steps=879, speed=68.847 frames/s, elapsed=0:01:14.829362\n",
      "Episode 15: reward=-21.0, steps=866, speed=68.834 frames/s, elapsed=0:01:27.525004\n",
      "Episode 16: reward=-20.0, steps=1199, speed=68.818 frames/s, elapsed=0:01:45.151744\n",
      "Episode 17: reward=-20.0, steps=972, speed=68.761 frames/s, elapsed=0:01:59.888782\n",
      "Episode 18: reward=-19.0, steps=1010, speed=68.734 frames/s, elapsed=0:02:14.869340\n",
      "Episode 19: reward=-20.0, steps=943, speed=68.681 frames/s, elapsed=0:02:29.139756\n",
      "Episode 20: reward=-21.0, steps=755, speed=68.625 frames/s, elapsed=0:02:40.602488\n",
      "Episode 21: reward=-20.0, steps=952, speed=68.629 frames/s, elapsed=0:02:54.433675\n",
      "Episode 22: reward=-21.0, steps=883, speed=68.604 frames/s, elapsed=0:03:07.531293\n",
      "Episode 23: reward=-19.0, steps=1207, speed=68.515 frames/s, elapsed=0:03:26.352312\n",
      "Episode 24: reward=-21.0, steps=760, speed=68.426 frames/s, elapsed=0:03:38.219495\n",
      "Episode 25: reward=-20.0, steps=913, speed=68.317 frames/s, elapsed=0:03:52.717297\n",
      "Episode 26: reward=-21.0, steps=757, speed=68.117 frames/s, elapsed=0:04:05.694415\n",
      "Episode 27: reward=-21.0, steps=847, speed=68.036 frames/s, elapsed=0:04:18.912086\n",
      "Episode 28: reward=-21.0, steps=839, speed=68.060 frames/s, elapsed=0:04:31.034769\n",
      "Episode 29: reward=-21.0, steps=1108, speed=68.117 frames/s, elapsed=0:04:46.658265\n",
      "Episode 30: reward=-20.0, steps=835, speed=68.160 frames/s, elapsed=0:04:58.538307\n",
      "Episode 31: reward=-20.0, steps=868, speed=68.216 frames/s, elapsed=0:05:10.767978\n",
      "Episode 32: reward=-20.0, steps=1067, speed=68.253 frames/s, elapsed=0:05:26.002346\n",
      "Episode 33: reward=-21.0, steps=784, speed=68.267 frames/s, elapsed=0:05:37.371779\n",
      "Episode 34: reward=-21.0, steps=883, speed=68.316 frames/s, elapsed=0:05:49.858852\n",
      "Episode 35: reward=-21.0, steps=877, speed=68.372 frames/s, elapsed=0:06:02.189792\n",
      "Episode 36: reward=-21.0, steps=778, speed=68.413 frames/s, elapsed=0:06:13.237064\n",
      "Episode 37: reward=-19.0, steps=1007, speed=68.389 frames/s, elapsed=0:06:28.217962\n",
      "Episode 38: reward=-20.0, steps=972, speed=68.064 frames/s, elapsed=0:06:46.868913\n",
      "Episode 39: reward=-20.0, steps=901, speed=67.876 frames/s, elapsed=0:07:02.223509\n",
      "Episode 40: reward=-19.0, steps=996, speed=67.772 frames/s, elapsed=0:07:18.115839\n",
      "Episode 41: reward=-21.0, steps=942, speed=67.624 frames/s, elapsed=0:07:33.725413\n",
      "Episode 42: reward=-21.0, steps=868, speed=67.565 frames/s, elapsed=0:07:47.142187\n",
      "Episode 43: reward=-20.0, steps=849, speed=67.550 frames/s, elapsed=0:07:59.848587\n",
      "Episode 44: reward=-19.0, steps=1227, speed=67.483 frames/s, elapsed=0:08:18.962184\n",
      "Episode 45: reward=-21.0, steps=906, speed=67.454 frames/s, elapsed=0:08:32.684309\n",
      "Episode 46: reward=-19.0, steps=961, speed=67.421 frames/s, elapsed=0:08:47.283646\n",
      "Episode 47: reward=-19.0, steps=946, speed=67.396 frames/s, elapsed=0:09:01.577810\n",
      "Episode 48: reward=-21.0, steps=939, speed=67.341 frames/s, elapsed=0:09:16.108323\n",
      "Episode 49: reward=-21.0, steps=1029, speed=67.305 frames/s, elapsed=0:09:31.809398\n",
      "Episode 50: reward=-20.0, steps=922, speed=67.386 frames/s, elapsed=0:09:44.724632\n",
      "Episode 51: reward=-21.0, steps=866, speed=67.435 frames/s, elapsed=0:09:57.131668\n",
      "Episode 52: reward=-20.0, steps=1023, speed=67.488 frames/s, elapsed=0:10:11.728232\n",
      "Episode 53: reward=-21.0, steps=896, speed=67.409 frames/s, elapsed=0:10:25.832090\n",
      "Episode 54: reward=-20.0, steps=895, speed=67.300 frames/s, elapsed=0:10:40.271906\n",
      "Episode 55: reward=-21.0, steps=802, speed=67.339 frames/s, elapsed=0:10:51.854551\n",
      "Episode 56: reward=-19.0, steps=959, speed=67.269 frames/s, elapsed=0:11:06.877082\n",
      "Episode 57: reward=-18.0, steps=1091, speed=67.246 frames/s, elapsed=0:11:23.369443\n",
      "Episode 58: reward=-21.0, steps=968, speed=67.288 frames/s, elapsed=0:11:37.332737\n",
      "Episode 59: reward=-21.0, steps=882, speed=67.368 frames/s, elapsed=0:11:49.705016\n",
      "Episode 60: reward=-21.0, steps=761, speed=67.378 frames/s, elapsed=0:12:00.918808\n",
      "Episode 61: reward=-21.0, steps=819, speed=67.295 frames/s, elapsed=0:12:13.869709\n",
      "Episode 62: reward=-21.0, steps=814, speed=67.290 frames/s, elapsed=0:12:26.009334\n",
      "Episode 63: reward=-21.0, steps=1087, speed=67.308 frames/s, elapsed=0:12:41.954440\n",
      "Episode 64: reward=-19.0, steps=974, speed=67.297 frames/s, elapsed=0:12:56.539488\n",
      "Episode 65: reward=-21.0, steps=837, speed=67.348 frames/s, elapsed=0:13:08.528993\n",
      "Episode 66: reward=-21.0, steps=821, speed=67.325 frames/s, elapsed=0:13:20.924556\n",
      "Episode 67: reward=-19.0, steps=1016, speed=67.240 frames/s, elapsed=0:13:37.033368\n",
      "Episode 68: reward=-20.0, steps=881, speed=67.257 frames/s, elapsed=0:13:49.977157\n",
      "Episode 69: reward=-20.0, steps=899, speed=67.203 frames/s, elapsed=0:14:03.904455\n",
      "Episode 70: reward=-21.0, steps=821, speed=67.151 frames/s, elapsed=0:14:16.610937\n",
      "Episode 71: reward=-21.0, steps=788, speed=67.151 frames/s, elapsed=0:14:28.344517\n",
      "Episode 72: reward=-21.0, steps=787, speed=67.150 frames/s, elapsed=0:14:40.071791\n",
      "Episode 73: reward=-21.0, steps=761, speed=67.135 frames/s, elapsed=0:14:51.534709\n",
      "Episode 74: reward=-21.0, steps=839, speed=67.076 frames/s, elapsed=0:15:04.606057\n",
      "Episode 75: reward=-21.0, steps=760, speed=67.076 frames/s, elapsed=0:15:15.933995\n",
      "Episode 76: reward=-21.0, steps=818, speed=67.070 frames/s, elapsed=0:15:28.184235\n",
      "Episode 77: reward=-21.0, steps=818, speed=67.087 frames/s, elapsed=0:15:40.228169\n",
      "Episode 78: reward=-20.0, steps=889, speed=67.138 frames/s, elapsed=0:15:52.993320\n",
      "Episode 79: reward=-20.0, steps=888, speed=67.158 frames/s, elapsed=0:16:06.024154\n",
      "Episode 80: reward=-21.0, steps=774, speed=67.141 frames/s, elapsed=0:16:17.702613\n",
      "Episode 81: reward=-19.0, steps=911, speed=67.146 frames/s, elapsed=0:16:31.219022\n",
      "Episode 82: reward=-21.0, steps=1042, speed=67.169 frames/s, elapsed=0:16:46.474670\n",
      "Episode 83: reward=-21.0, steps=928, speed=67.185 frames/s, elapsed=0:17:00.131688\n",
      "Episode 84: reward=-21.0, steps=816, speed=67.216 frames/s, elapsed=0:17:12.004667\n",
      "Episode 85: reward=-21.0, steps=909, speed=67.223 frames/s, elapsed=0:17:25.457016\n",
      "Episode 86: reward=-20.0, steps=925, speed=67.212 frames/s, elapsed=0:17:39.330648\n",
      "Episode 87: reward=-21.0, steps=818, speed=67.209 frames/s, elapsed=0:17:51.525099\n",
      "Episode 88: reward=-21.0, steps=817, speed=67.126 frames/s, elapsed=0:18:04.477221\n",
      "Episode 89: reward=-20.0, steps=943, speed=67.111 frames/s, elapsed=0:18:18.685426\n",
      "Episode 90: reward=-21.0, steps=806, speed=67.110 frames/s, elapsed=0:18:30.704270\n",
      "Episode 91: reward=-21.0, steps=906, speed=67.027 frames/s, elapsed=0:18:45.096041\n",
      "Episode 92: reward=-21.0, steps=756, speed=67.003 frames/s, elapsed=0:18:56.585441\n",
      "Episode 93: reward=-21.0, steps=821, speed=67.001 frames/s, elapsed=0:19:08.854074\n",
      "Episode 94: reward=-21.0, steps=759, speed=67.000 frames/s, elapsed=0:19:20.192312\n",
      "Episode 95: reward=-20.0, steps=862, speed=67.003 frames/s, elapsed=0:19:33.030282\n",
      "Episode 96: reward=-21.0, steps=759, speed=67.006 frames/s, elapsed=0:19:44.327443\n",
      "Episode 97: reward=-21.0, steps=895, speed=67.020 frames/s, elapsed=0:19:57.552923\n",
      "Episode 98: reward=-21.0, steps=838, speed=67.012 frames/s, elapsed=0:20:10.124730\n",
      "Episode 99: reward=-21.0, steps=966, speed=66.987 frames/s, elapsed=0:20:24.815371\n",
      "Episode 100: reward=-18.0, steps=1042, speed=66.994 frames/s, elapsed=0:20:40.296405\n",
      "Episode 101: reward=-20.0, steps=923, speed=66.995 frames/s, elapsed=0:20:54.064294\n",
      "Episode 102: reward=-21.0, steps=820, speed=67.034 frames/s, elapsed=0:21:05.954145\n",
      "Episode 103: reward=-21.0, steps=939, speed=67.073 frames/s, elapsed=0:21:19.566689\n",
      "Episode 104: reward=-21.0, steps=762, speed=67.048 frames/s, elapsed=0:21:31.144331\n",
      "Episode 105: reward=-21.0, steps=963, speed=67.077 frames/s, elapsed=0:21:45.199124\n",
      "Episode 106: reward=-21.0, steps=784, speed=67.099 frames/s, elapsed=0:21:56.697068\n",
      "Episode 107: reward=-20.0, steps=958, speed=67.140 frames/s, elapsed=0:22:10.553650\n",
      "Episode 108: reward=-21.0, steps=815, speed=67.189 frames/s, elapsed=0:22:22.263726\n",
      "Episode 109: reward=-21.0, steps=760, speed=67.190 frames/s, elapsed=0:22:33.568579\n",
      "Episode 110: reward=-21.0, steps=819, speed=67.190 frames/s, elapsed=0:22:45.754966\n",
      "Episode 111: reward=-21.0, steps=902, speed=67.192 frames/s, elapsed=0:22:59.164829\n",
      "Episode 112: reward=-21.0, steps=927, speed=67.210 frames/s, elapsed=0:23:12.773811\n",
      "Episode 113: reward=-21.0, steps=757, speed=67.234 frames/s, elapsed=0:23:23.845102\n",
      "Episode 114: reward=-21.0, steps=903, speed=67.281 frames/s, elapsed=0:23:36.819205\n",
      "Episode 115: reward=-21.0, steps=819, speed=67.312 frames/s, elapsed=0:23:48.717900\n",
      "Episode 116: reward=-21.0, steps=822, speed=67.328 frames/s, elapsed=0:24:00.788922\n",
      "Episode 117: reward=-21.0, steps=873, speed=67.316 frames/s, elapsed=0:24:13.867111\n",
      "Episode 118: reward=-21.0, steps=790, speed=67.372 frames/s, elapsed=0:24:25.138089\n",
      "Episode 119: reward=-21.0, steps=757, speed=67.346 frames/s, elapsed=0:24:36.590290\n",
      "Episode 120: reward=-21.0, steps=761, speed=67.315 frames/s, elapsed=0:24:48.158653\n",
      "Episode 121: reward=-21.0, steps=785, speed=67.302 frames/s, elapsed=0:24:59.933821\n",
      "Episode 122: reward=-21.0, steps=806, speed=67.325 frames/s, elapsed=0:25:11.704832\n",
      "Episode 123: reward=-21.0, steps=761, speed=67.336 frames/s, elapsed=0:25:22.917569\n",
      "Episode 124: reward=-21.0, steps=755, speed=67.328 frames/s, elapsed=0:25:34.196504\n",
      "Episode 125: reward=-21.0, steps=757, speed=67.296 frames/s, elapsed=0:25:45.715453\n",
      "Episode 126: reward=-21.0, steps=787, speed=67.294 frames/s, elapsed=0:25:57.431072\n",
      "Episode 127: reward=-21.0, steps=758, speed=67.303 frames/s, elapsed=0:26:08.618115\n",
      "Episode 128: reward=-21.0, steps=756, speed=67.271 frames/s, elapsed=0:26:20.124110\n",
      "Episode 129: reward=-21.0, steps=845, speed=67.295 frames/s, elapsed=0:26:32.463605\n",
      "Episode 130: reward=-21.0, steps=762, speed=67.299 frames/s, elapsed=0:26:43.753573\n",
      "Episode 131: reward=-21.0, steps=805, speed=67.274 frames/s, elapsed=0:26:55.945831\n",
      "Episode 132: reward=-21.0, steps=806, speed=67.238 frames/s, elapsed=0:27:08.249424\n",
      "Episode 133: reward=-21.0, steps=756, speed=67.186 frames/s, elapsed=0:27:19.947697\n",
      "Episode 134: reward=-21.0, steps=819, speed=67.158 frames/s, elapsed=0:27:32.402572\n",
      "Episode 135: reward=-20.0, steps=830, speed=67.160 frames/s, elapsed=0:27:44.736999\n",
      "Episode 136: reward=-20.0, steps=895, speed=67.192 frames/s, elapsed=0:27:57.759665\n",
      "Episode 137: reward=-21.0, steps=818, speed=67.209 frames/s, elapsed=0:28:09.779120\n",
      "Episode 138: reward=-21.0, steps=784, speed=67.208 frames/s, elapsed=0:28:21.449176\n",
      "Episode 139: reward=-21.0, steps=823, speed=67.148 frames/s, elapsed=0:28:34.274937\n",
      "Episode 140: reward=-20.0, steps=921, speed=67.147 frames/s, elapsed=0:28:47.999268\n",
      "Episode 141: reward=-21.0, steps=846, speed=67.148 frames/s, elapsed=0:29:00.585213\n",
      "Episode 142: reward=-20.0, steps=868, speed=67.146 frames/s, elapsed=0:29:13.535569\n",
      "Episode 143: reward=-21.0, steps=757, speed=67.157 frames/s, elapsed=0:29:24.718221\n",
      "Episode 144: reward=-21.0, steps=820, speed=67.143 frames/s, elapsed=0:29:37.053111\n",
      "Episode 145: reward=-21.0, steps=758, speed=67.126 frames/s, elapsed=0:29:48.484396\n",
      "Episode 146: reward=-21.0, steps=784, speed=67.034 frames/s, elapsed=0:30:01.026993\n",
      "Episode 147: reward=-21.0, steps=760, speed=67.084 frames/s, elapsed=0:30:11.959991\n",
      "Episode 148: reward=-21.0, steps=818, speed=67.088 frames/s, elapsed=0:30:24.116002\n",
      "Episode 149: reward=-21.0, steps=760, speed=67.108 frames/s, elapsed=0:30:35.280356\n",
      "Episode 150: reward=-21.0, steps=775, speed=67.112 frames/s, elapsed=0:30:46.793520\n",
      "Episode 151: reward=-21.0, steps=756, speed=67.103 frames/s, elapsed=0:30:58.133604\n",
      "Episode 152: reward=-21.0, steps=847, speed=67.089 frames/s, elapsed=0:31:10.888438\n",
      "Episode 153: reward=-21.0, steps=762, speed=67.105 frames/s, elapsed=0:31:22.109939\n",
      "Episode 154: reward=-21.0, steps=758, speed=67.166 frames/s, elapsed=0:31:32.916942\n",
      "Episode 155: reward=-21.0, steps=756, speed=66.959 frames/s, elapsed=0:31:46.218053\n",
      "Episode 156: reward=-21.0, steps=761, speed=66.813 frames/s, elapsed=0:31:58.971039\n",
      "Episode 157: reward=-20.0, steps=838, speed=66.746 frames/s, elapsed=0:32:12.181796\n",
      "Episode 158: reward=-21.0, steps=755, speed=66.644 frames/s, elapsed=0:32:24.424196\n",
      "Episode 159: reward=-21.0, steps=799, speed=66.541 frames/s, elapsed=0:32:37.416588\n",
      "Episode 160: reward=-21.0, steps=847, speed=66.472 frames/s, elapsed=0:32:50.845922\n",
      "Episode 161: reward=-20.0, steps=857, speed=66.355 frames/s, elapsed=0:33:04.979520\n",
      "Episode 162: reward=-21.0, steps=943, speed=66.267 frames/s, elapsed=0:33:20.200514\n",
      "Episode 163: reward=-21.0, steps=817, speed=66.171 frames/s, elapsed=0:33:33.497735\n",
      "Episode 164: reward=-21.0, steps=823, speed=66.050 frames/s, elapsed=0:33:47.182361\n",
      "Episode 165: reward=-20.0, steps=832, speed=65.939 frames/s, elapsed=0:34:00.932425\n",
      "Episode 166: reward=-21.0, steps=816, speed=65.848 frames/s, elapsed=0:34:14.231079\n",
      "Episode 167: reward=-21.0, steps=759, speed=65.731 frames/s, elapsed=0:34:26.877793\n",
      "Episode 168: reward=-20.0, steps=840, speed=65.646 frames/s, elapsed=0:34:40.546006\n",
      "Episode 169: reward=-21.0, steps=759, speed=65.579 frames/s, elapsed=0:34:52.730490\n",
      "Episode 170: reward=-21.0, steps=790, speed=65.509 frames/s, elapsed=0:35:05.454182\n",
      "Episode 171: reward=-21.0, steps=786, speed=65.361 frames/s, elapsed=0:35:18.980893\n",
      "Episode 172: reward=-21.0, steps=790, speed=65.195 frames/s, elapsed=0:35:32.825588\n",
      "Episode 173: reward=-20.0, steps=836, speed=65.065 frames/s, elapsed=0:35:47.063412\n",
      "Episode 174: reward=-21.0, steps=784, speed=64.945 frames/s, elapsed=0:36:00.339401\n",
      "Episode 175: reward=-20.0, steps=835, speed=64.816 frames/s, elapsed=0:36:14.618009\n",
      "Episode 176: reward=-20.0, steps=886, speed=64.697 frames/s, elapsed=0:36:29.666102\n",
      "Episode 177: reward=-21.0, steps=761, speed=64.580 frames/s, elapsed=0:36:42.592315\n",
      "Episode 178: reward=-21.0, steps=818, speed=64.470 frames/s, elapsed=0:36:56.445476\n",
      "Episode 179: reward=-21.0, steps=756, speed=64.352 frames/s, elapsed=0:37:09.347073\n",
      "Episode 180: reward=-21.0, steps=784, speed=64.225 frames/s, elapsed=0:37:22.868438\n",
      "Episode 181: reward=-21.0, steps=759, speed=64.114 frames/s, elapsed=0:37:35.806794\n",
      "Episode 182: reward=-21.0, steps=759, speed=64.012 frames/s, elapsed=0:37:48.668169\n",
      "Episode 183: reward=-21.0, steps=758, speed=63.910 frames/s, elapsed=0:38:01.536496\n",
      "Episode 184: reward=-21.0, steps=785, speed=63.815 frames/s, elapsed=0:38:14.801346\n",
      "Episode 185: reward=-21.0, steps=816, speed=63.668 frames/s, elapsed=0:38:29.248412\n",
      "Episode 186: reward=-21.0, steps=761, speed=63.560 frames/s, elapsed=0:38:42.312895\n",
      "Episode 187: reward=-21.0, steps=776, speed=63.458 frames/s, elapsed=0:38:55.587298\n",
      "Episode 188: reward=-20.0, steps=865, speed=63.354 frames/s, elapsed=0:39:10.435651\n",
      "Episode 189: reward=-21.0, steps=756, speed=63.259 frames/s, elapsed=0:39:23.331069\n",
      "Episode 190: reward=-21.0, steps=755, speed=63.157 frames/s, elapsed=0:39:36.317122\n",
      "Episode 191: reward=-21.0, steps=787, speed=63.075 frames/s, elapsed=0:39:49.642922\n",
      "Episode 192: reward=-21.0, steps=815, speed=62.998 frames/s, elapsed=0:40:03.406399\n",
      "Episode 193: reward=-21.0, steps=760, speed=62.876 frames/s, elapsed=0:40:16.760961\n",
      "Episode 194: reward=-21.0, steps=755, speed=62.801 frames/s, elapsed=0:40:29.532708\n",
      "Episode 195: reward=-20.0, steps=837, speed=62.708 frames/s, elapsed=0:40:43.916266\n",
      "Episode 196: reward=-21.0, steps=808, speed=62.631 frames/s, elapsed=0:40:57.655358\n",
      "Episode 197: reward=-20.0, steps=833, speed=62.554 frames/s, elapsed=0:41:11.816470\n",
      "Episode 198: reward=-21.0, steps=759, speed=62.486 frames/s, elapsed=0:41:24.656554\n",
      "Episode 199: reward=-20.0, steps=903, speed=62.398 frames/s, elapsed=0:41:40.197617\n",
      "Episode 200: reward=-21.0, steps=816, speed=62.326 frames/s, elapsed=0:41:54.077714\n",
      "Episode 201: reward=-20.0, steps=872, speed=62.260 frames/s, elapsed=0:42:08.851144\n",
      "Episode 202: reward=-21.0, steps=755, speed=62.197 frames/s, elapsed=0:42:21.622709\n",
      "Episode 203: reward=-21.0, steps=822, speed=62.143 frames/s, elapsed=0:42:35.442496\n",
      "Episode 204: reward=-21.0, steps=803, speed=62.145 frames/s, elapsed=0:42:48.340254\n",
      "Episode 205: reward=-21.0, steps=761, speed=62.277 frames/s, elapsed=0:42:59.406745\n",
      "Episode 206: reward=-21.0, steps=878, speed=62.408 frames/s, elapsed=0:43:12.163413\n",
      "Episode 207: reward=-21.0, steps=838, speed=62.542 frames/s, elapsed=0:43:24.294948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Engine run is terminating due to exception: .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13612\\3654009473.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_handler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mptan_ignite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPeriodEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERS_100_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_internal_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Engine run is terminating due to exception: %s.\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataloader_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_handle_exception\u001b[1;34m(self, e)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXCEPTION_RAISED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_internal_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                 \u001b[0mtime_taken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_once_on_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m                 \u001b[1;31m# time is available for handlers but must be update after fire\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEPOCH_COMPLETED\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\ignite\\engine\\engine.py\u001b[0m in \u001b[0;36m_run_once_on_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    776\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fire_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mITERATION_COMPLETED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13612\\3245619744.py\u001b[0m in \u001b[0;36mprocess_batch\u001b[1;34m(engine, batch)\u001b[0m\n\u001b[0;32m      3\u001b[0m     loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model,\n\u001b[0;32m      4\u001b[0m                                   gamma=params.gamma, device=device)\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mloss_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mepsilon_tracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\aiprojects\\deeprl\\deeprl\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "def episode_completed(trainer: Engine):\n",
    "    print(\"Episode %d: reward=%s, steps=%s, speed=%.3f frames/s, elapsed=%s\" % (\n",
    "        trainer.state.episode, trainer.state.episode_reward,\n",
    "        trainer.state.episode_steps, trainer.state.metrics.get('avg_fps', 0),\n",
    "        timedelta(seconds=trainer.state.metrics.get('time_passed', 0))))\n",
    "    \n",
    "@engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "def game_solved(trainer: Engine):\n",
    "    print(\"Game solved in %s, after %d episodes and %d iterations!\" % (\n",
    "        timedelta(seconds=trainer.state.metrics['time_passed']),\n",
    "        trainer.state.episode, trainer.state.iteration))\n",
    "    trainer.should_terminate = True\n",
    "    \n",
    "logdir = f\"runs/{datetime.now().minute}-{params.run_name}-{NAME}\"\n",
    "tb = tb_logger.TensorboardLogger(log_dir=logdir)\n",
    "RunningAverage(output_transform=lambda v: v['loss']).attach(engine, \"avg_loss\")\n",
    "\n",
    "episode_handler = tb_logger.OutputHandler(tag='episodes', metric_names=['reward', 'steps', 'avg_reward'])\n",
    "tb.attach(engine, log_handler=episode_handler, event_name=ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "\n",
    "# writing to tensorboard every 100 iterations\n",
    "ptan_ignite.PeriodicEvents().attach(engine)\n",
    "handler = tb_logger.OutputHandler(tag=\"train\", metric_names=['avg_loss', 'avg_fps'],\n",
    "                                  output_transform=lambda a: a)\n",
    "tb.attach(engine, log_handler=handler, event_name=ptan_ignite.PeriodEvents.ITERS_100_COMPLETED)\n",
    "\n",
    "engine.run(batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f628a-9eed-49c0-8125-530074e4b1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
