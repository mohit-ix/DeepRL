{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb17d43-51fd-4809-b1ab-5072ca899a7c",
   "metadata": {},
   "source": [
    "## Cartpole\n",
    "In chapter 4, we will apply Cross Entropy method on Environment of Cartpole and FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8f74f-eaff-4c74-a676-9b11feef42ad",
   "metadata": {},
   "source": [
    "Cross Entropy is a model-free, policy-based and on-policy method. This means that:\n",
    "1. It does not build a model for the environment, it tells agent to do every step.\n",
    "2. It approximates the policy of the agent. Policy is usually described as probability distribution.\n",
    "3. It only requires fresh data taken from the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58e2659-ed80-41d8-92d7-54bce6fc0211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gym is used to initial environment, taking action and getting observation and reward.\n",
    "import gym\n",
    "# Namedtuple is used to create custom tuple with pre-defined names\n",
    "from collections import namedtuple\n",
    "# Numpy is used to initialize and use fucntions on array\n",
    "import numpy as np\n",
    "# For keeping logs\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# For using Pytorch\n",
    "import torch\n",
    "# For creating a neural network\n",
    "import torch.nn as nn\n",
    "# For using optimizers\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49aa85c8-fd2d-4f09-8482-4793f7da9e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No. of neurons in the hidden layer\n",
    "HIDDEN_SIZE = 128\n",
    "# No. of episodes in each epoch\n",
    "BATCH_SIZE = 16\n",
    "# It shows that top 30% result will be used for training\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fa37f4a-ab98-4554-bd4e-b384459c263a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''\n",
    "    Creating a neural network\n",
    "    '''\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63903ebe-2ff9-41b8-9265-333bef6dbba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating named tuples\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    '''\n",
    "    This is a generator fuction that provide infinite observations of batch_size\n",
    "    '''\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done:\n",
    "            e = Episode(reward = episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d00b2a-3ed3-4077-bff6-68987fe87488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    '''\n",
    "    From all the episodes in a batch selecting top 30% for training\n",
    "    '''\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    \n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "    \n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e076d50-f228-4a3e-97cd-dadc7cecffa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_size:  4\n",
      "Actions:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env, directory=\"CartPole-v0\", force=True)\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(\"observation_size: \", obs_size)\n",
    "print(\"Actions: \", n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "232bc0bd-6c8c-43e3-8fb7-2d38b3339819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr = 0.01)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5f4d8e4-3228-4f1d-8bcb-0c1d6788e438",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.668, reward_mean=25.9, rw_bound=28.5\n",
      "1: loss=0.667, reward_mean=29.0, rw_bound=26.5\n",
      "2: loss=0.654, reward_mean=32.5, rw_bound=42.5\n",
      "3: loss=0.646, reward_mean=36.8, rw_bound=46.5\n",
      "4: loss=0.648, reward_mean=46.6, rw_bound=57.0\n",
      "5: loss=0.637, reward_mean=62.2, rw_bound=64.0\n",
      "6: loss=0.644, reward_mean=35.1, rw_bound=40.5\n",
      "7: loss=0.621, reward_mean=48.4, rw_bound=56.0\n",
      "8: loss=0.614, reward_mean=48.8, rw_bound=51.5\n",
      "9: loss=0.616, reward_mean=68.7, rw_bound=96.0\n",
      "10: loss=0.599, reward_mean=66.5, rw_bound=86.0\n",
      "11: loss=0.618, reward_mean=70.2, rw_bound=79.5\n",
      "12: loss=0.571, reward_mean=83.6, rw_bound=90.0\n",
      "13: loss=0.594, reward_mean=88.6, rw_bound=105.5\n",
      "14: loss=0.586, reward_mean=84.9, rw_bound=95.5\n",
      "15: loss=0.571, reward_mean=100.0, rw_bound=114.5\n",
      "16: loss=0.562, reward_mean=87.1, rw_bound=102.0\n",
      "17: loss=0.555, reward_mean=80.3, rw_bound=94.0\n",
      "18: loss=0.547, reward_mean=117.1, rw_bound=152.0\n",
      "19: loss=0.558, reward_mean=122.6, rw_bound=143.0\n",
      "20: loss=0.535, reward_mean=127.8, rw_bound=154.0\n",
      "21: loss=0.548, reward_mean=126.2, rw_bound=144.5\n",
      "22: loss=0.541, reward_mean=128.8, rw_bound=157.0\n",
      "23: loss=0.540, reward_mean=113.6, rw_bound=129.0\n",
      "24: loss=0.538, reward_mean=123.4, rw_bound=134.5\n",
      "25: loss=0.533, reward_mean=131.6, rw_bound=139.5\n",
      "26: loss=0.525, reward_mean=136.6, rw_bound=158.0\n",
      "27: loss=0.516, reward_mean=156.8, rw_bound=200.0\n",
      "28: loss=0.512, reward_mean=140.2, rw_bound=159.0\n",
      "29: loss=0.523, reward_mean=137.8, rw_bound=167.5\n",
      "30: loss=0.507, reward_mean=158.9, rw_bound=191.0\n",
      "31: loss=0.490, reward_mean=165.1, rw_bound=200.0\n",
      "32: loss=0.490, reward_mean=155.3, rw_bound=186.5\n",
      "33: loss=0.488, reward_mean=165.6, rw_bound=200.0\n",
      "34: loss=0.499, reward_mean=172.8, rw_bound=200.0\n",
      "35: loss=0.484, reward_mean=170.8, rw_bound=198.5\n",
      "36: loss=0.494, reward_mean=181.4, rw_bound=200.0\n",
      "37: loss=0.474, reward_mean=168.6, rw_bound=200.0\n",
      "38: loss=0.497, reward_mean=192.9, rw_bound=200.0\n",
      "39: loss=0.483, reward_mean=194.1, rw_bound=200.0\n",
      "40: loss=0.500, reward_mean=200.0, rw_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    if reward_m > 199:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c404c2-8995-45d1-b518-299205a334f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
